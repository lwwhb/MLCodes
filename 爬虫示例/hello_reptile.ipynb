{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.爬虫分类\n",
    "   * 通用爬虫 (搜索引擎)\n",
    "       搜索引擎如何获取新的url\n",
    "           1. 新网站主动提交到搜索引擎\n",
    "           2. 通过其他网站设置的外链\n",
    "           3. 搜索引擎和DNS服务商合作，获取最新收录的网站\n",
    "   * 聚焦爬虫（特定领域数据爬取程序）\n",
    "   \n",
    "### 2.Robots协议\n",
    "    网站指定一个Robots.txt，告诉爬虫引擎什么可以爬取，如淘宝网站：https://www.taobao.com/robots.txt  东方财富：https://www.eastmoney.com/robots.txt, 但一般网站会提供Sitemap网站地图告诉爬虫引擎哪些可以爬取。Robots协议是个君子协议，往往还会暴露出网站关键内容的网址。撰写爬虫程序前可以加以了解。\n",
    "\n",
    "### 3 urllib库\n",
    "    urllib是标准库，它是一个工具包模块，包含下面模块来处理url\n",
    "   * urllib.request 用于打开和读取url\n",
    "   * urllib.error包含了由urllib.request引起的一场\n",
    "   * urllib.parse用于解析url\n",
    "   * urllib.robotparser分析robots.txt文件\n",
    "    Python2提供了urllib和urllib2。urllib提供较为底层接口，urllib2对urllib进行了封装.Python3中将urllib2合并到urllib中，只提供urllib的标准库。\n",
    "    \n",
    "#### 3.1 urllib.request模块\n",
    "    模块定义了基本和摘要式身份验证、重定向、cookies等应用中打开url,主要是http的函数和类\n",
    "   #### urlopen方法\n",
    "    urllib.request.urlopen(url, data = None,[timeout,]*,cafile = None,capath = None, cadefault = False, context = None) 其中data=None发起Get请求，否则发起Post请求，urlpen返回http.HTTPResponse类\n",
    "   #### User-Agent问题\n",
    "    对于有些反爬虫网站，需要通过User-Agent进行浏览器伪装。\n",
    "   #### Request类\n",
    "    Request(url, data=None, headers{}) 可以通过headers字典添加键值对。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen,Request\n",
    "import random\n",
    "\n",
    "url = 'https://www.eastmoney.com'\n",
    "user_agent_list = ['Mozilla/5.0 (Windows NT 6.2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/28.0.1464.0 Safari/537.36',\n",
    "                       'Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/31.0.1650.16 Safari/537.36',\n",
    "                       'Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.3319.102 Safari/537.36',\n",
    "                       'Mozilla/5.0 (X11; CrOS i686 3912.101.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.116 Safari/537.36',\n",
    "                       'Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.93 Safari/537.36',\n",
    "                       'Mozilla/5.0 (Windows NT 6.2; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1667.0 Safari/537.36',\n",
    "                       'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:17.0) Gecko/20100101 Firefox/17.0.6',\n",
    "                       'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/28.0.1468.0 Safari/537.36',\n",
    "                       'Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2224.3 Safari/537.36',\n",
    "                       'Mozilla/5.0 (X11; CrOS i686 3912.101.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.116 Safari/537.36',\n",
    "                       'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.61 Safari/537.36']\n",
    "# 随机伪装user-agent\n",
    "ua=random.choice(user_agent_list)\n",
    "  \n",
    "#request = Request(url, headers={\n",
    "#    'User-agent':ua\n",
    "#})\n",
    "#或用下面方法添加User-agent\n",
    "request.add_header('User-agent', ua)\n",
    "\n",
    "#response = urlopen(url)\n",
    "response = urlopen(request)\n",
    "print(response.closed)\n",
    "\n",
    "with response:\n",
    "    print(0,type(response))\n",
    "    print(1,response.status, response.getcode(), response.reason)\n",
    "    print(2,response._method)\n",
    "    print(3,response.info())\n",
    "   # print(response.read())\n",
    "    print(4,response.geturl())\n",
    "    print(5,request.get_header('User-agent'))\n",
    "    print(6,'User-agent'.capitalize())\n",
    "print(response.closed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 urllib中parse模块\n",
    "    此模块用于对url进行编解码\n",
    "   ##### Https访问问题\n",
    "   加载SSL模块  通过context = ssl._create_unverified_context()  response = urlopen(request, context = context) #忽略校验证书"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import parse\n",
    "#编码\n",
    "u = parse.urlencode({'wd':\"爬虫\"})\n",
    "url = \"https://www.baidu.com/s?{}\".format(u)\n",
    "print(url)\n",
    "\n",
    "print('爬虫'.encode('utf-8'))\n",
    "print(parse.unquote(u))\n",
    "print(parse.unquote(url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. urllib3包\n",
    "    标准库urllib包缺少一些关键性功能，非标准库urllib3包提供了这些功能，如连接池管理等 通过pip install urllib3安装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib3\n",
    "\n",
    "url = 'https://movie.douban.com'\n",
    "ua = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.61 Safari/537.36'\n",
    "\n",
    "with urllib3.PoolManager() as http:\n",
    "    response = http.request('GET',url,headers={\n",
    "        'User-agent':ua\n",
    "    })\n",
    "    print(0, type(response))\n",
    "    print(1,response.status, response.reason)\n",
    "    print(2,response.headers)\n",
    "    #print(3,response.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. requests库\n",
    "    使用了urllib3库，API更加友好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from urllib import parse\n",
    "jurl = 'https://movie.douban.com/j/search_subjects'\n",
    "d = {\n",
    "    'type':'movie',\n",
    "    'tag':'热门',\n",
    "    'page_limit':10,\n",
    "    'page_start':10\n",
    "}\n",
    "url = '{}?{}'.format(jurl, parse.urlencode(d))\n",
    "ua = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.61 Safari/537.36'\n",
    "\n",
    "response = requests.request('GET',url,headers={\n",
    "        'User-agent':ua\n",
    "    })\n",
    "with response as resp:\n",
    "    print(resp.text)\n",
    "    print(resp.status_code)\n",
    "    print(resp.url)\n",
    "    print(resp.headers,\"~~~~\")\n",
    "    print(resp.request.headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "urls = ['https://www.baidu.com/s?wd=magedu','https://www.baidu.com/s?wd=magedu']\n",
    "ua = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.61 Safari/537.36'\n",
    "\n",
    "session = requests.Session()\n",
    "with session:\n",
    "    for url in urls :\n",
    "        response = session.get(url,headers={\n",
    "            'User-agent':ua\n",
    "        })\n",
    "        with response as resp:\n",
    "            print(resp.text[:50])\n",
    "            print(resp.status_code)\n",
    "            print(resp.url)\n",
    "            print('-'*30)\n",
    "            print(resp.cookies)\n",
    "            print('-'*30)\n",
    "            print(resp.headers,\"~~~~\")\n",
    "            print(resp.request.headers)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. HTML解析\n",
    "  #### 6.1 XPath *** \n",
    "      XPath是一门在XML文档中查找信息语言， Xpath可用来在XML文档中对元素和属性进行遍历\n",
    "  #### 6.2 lxml库\n",
    "      lxml是python下功能丰富的XML、HTML解析库，性能非常好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "隐秘的爱\n",
      "梦之城堡\n",
      "普拉吉布雷劳\n",
      "瘦身大作战\n",
      "你会在20岁时死去\n",
      "勒热夫战役\n",
      "圣·弗朗西斯\n",
      "杀不了的他与死不了的她\n",
      "闭锁病房\n",
      "兄弟会\n"
     ]
    }
   ],
   "source": [
    "from lxml import etree\n",
    "import requests\n",
    "\n",
    "urls = ['https://movie.douban.com/']\n",
    "ua = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.61 Safari/537.36'\n",
    "\n",
    "session = requests.Session()\n",
    "with session:\n",
    "    for url in urls :\n",
    "        response = session.get(url,headers={\n",
    "            'User-agent':ua\n",
    "        })\n",
    "        with response as resp:\n",
    "            content = resp.text\n",
    "            # xpath //div[@class='billboard-bd']//tr/td/a/text()\n",
    "            html = etree.HTML(content)\n",
    "            titles = html.xpath(\"//div[@class='billboard-bd']//tr/td/a/text()\")\n",
    "            for t in titles:\n",
    "                print(t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
