{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 神经网络实现鸢尾花分类\n",
    "* 准备数据\n",
    "    - 数据集读入\n",
    "    - 数据集乱序\n",
    "    - 生成训练集和测试集（即x_train/y_train, x_test/y_test）\n",
    "    - 配成（输入特征，标签）对，每次读入一小撮（batch）\n",
    "* 搭建网络\n",
    "    - 定义神经网络中所有可训练参数\n",
    "* 参数优化\n",
    "    - 嵌套循环迭代，with结构更新参数，显示当前loss\n",
    "* 测试效果\n",
    "    - 计算当前参数前向传播后的准确率，显示当前acc\n",
    "* acc / loss可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 0.2821310982108116\n",
      "Test_acc: 0.16666666666666666\n",
      "--------------------------\n",
      "Epoch 1, loss: 0.25459615513682365\n",
      "Test_acc: 0.16666666666666666\n",
      "--------------------------\n",
      "Epoch 2, loss: 0.22570249810814857\n",
      "Test_acc: 0.16666666666666666\n",
      "--------------------------\n",
      "Epoch 3, loss: 0.21028399839997292\n",
      "Test_acc: 0.16666666666666666\n",
      "--------------------------\n",
      "Epoch 4, loss: 0.19942265003919601\n",
      "Test_acc: 0.16666666666666666\n",
      "--------------------------\n",
      "Epoch 5, loss: 0.18873637914657593\n",
      "Test_acc: 0.5\n",
      "--------------------------\n",
      "Epoch 6, loss: 0.17851299047470093\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 7, loss: 0.16922875493764877\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 8, loss: 0.16107673197984695\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 9, loss: 0.15404684841632843\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 10, loss: 0.14802725985646248\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 11, loss: 0.14287303015589714\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 12, loss: 0.1384414155036211\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 13, loss: 0.13460607640445232\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 14, loss: 0.13126072846353054\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 15, loss: 0.12831822223961353\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 16, loss: 0.12570795230567455\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 17, loss: 0.12337299063801765\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 18, loss: 0.12126746587455273\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 19, loss: 0.11935433372855186\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 20, loss: 0.11760355532169342\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 21, loss: 0.11599067971110344\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 22, loss: 0.11449568346142769\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 23, loss: 0.11310207843780518\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 24, loss: 0.11179621890187263\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 25, loss: 0.11056671477854252\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 26, loss: 0.10940408334136009\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 27, loss: 0.10830028541386127\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 28, loss: 0.10724855214357376\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 29, loss: 0.10624313168227673\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 30, loss: 0.1052791029214859\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 31, loss: 0.10435221716761589\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 32, loss: 0.10345886647701263\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 33, loss: 0.10259587876498699\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 34, loss: 0.10176052898168564\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 35, loss: 0.10095042176544666\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 36, loss: 0.10016347281634808\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 37, loss: 0.09939785301685333\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 38, loss: 0.0986519306898117\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 39, loss: 0.09792429022490978\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 40, loss: 0.09721364825963974\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 41, loss: 0.09651889838278294\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 42, loss: 0.09583901055157185\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 43, loss: 0.09517310559749603\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 44, loss: 0.09452036581933498\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 45, loss: 0.0938800759613514\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 46, loss: 0.09325156174600124\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 47, loss: 0.09263424947857857\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 48, loss: 0.09202760085463524\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 49, loss: 0.09143111668527126\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 50, loss: 0.09084436297416687\n",
      "Test_acc: 0.5666666666666667\n",
      "--------------------------\n",
      "Epoch 51, loss: 0.09026693925261497\n",
      "Test_acc: 0.5666666666666667\n",
      "--------------------------\n",
      "Epoch 52, loss: 0.08969846926629543\n",
      "Test_acc: 0.5666666666666667\n",
      "--------------------------\n",
      "Epoch 53, loss: 0.08913860656321049\n",
      "Test_acc: 0.6\n",
      "--------------------------\n",
      "Epoch 54, loss: 0.08858705498278141\n",
      "Test_acc: 0.6\n",
      "--------------------------\n",
      "Epoch 55, loss: 0.08804351650178432\n",
      "Test_acc: 0.6\n",
      "--------------------------\n",
      "Epoch 56, loss: 0.08750772476196289\n",
      "Test_acc: 0.6\n",
      "--------------------------\n",
      "Epoch 57, loss: 0.0869794450700283\n",
      "Test_acc: 0.6\n",
      "--------------------------\n",
      "Epoch 58, loss: 0.08645843528211117\n",
      "Test_acc: 0.6\n",
      "--------------------------\n",
      "Epoch 59, loss: 0.08594449050724506\n",
      "Test_acc: 0.6\n",
      "--------------------------\n",
      "Epoch 60, loss: 0.08543741703033447\n",
      "Test_acc: 0.6\n",
      "--------------------------\n",
      "Epoch 61, loss: 0.08493701927363873\n",
      "Test_acc: 0.6\n",
      "--------------------------\n",
      "Epoch 62, loss: 0.08444313891232014\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 63, loss: 0.08395560272037983\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 64, loss: 0.08347426354885101\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 65, loss: 0.08299898356199265\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 66, loss: 0.08252961561083794\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 67, loss: 0.08206603862345219\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 68, loss: 0.08160812221467495\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 69, loss: 0.08115577697753906\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 70, loss: 0.08070887625217438\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 71, loss: 0.08026730827987194\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 72, loss: 0.07983098551630974\n",
      "Test_acc: 0.6666666666666666\n",
      "--------------------------\n",
      "Epoch 73, loss: 0.07939981296658516\n",
      "Test_acc: 0.6666666666666666\n",
      "--------------------------\n",
      "Epoch 74, loss: 0.0789736919105053\n",
      "Test_acc: 0.6666666666666666\n",
      "--------------------------\n",
      "Epoch 75, loss: 0.07855254225432873\n",
      "Test_acc: 0.7\n",
      "--------------------------\n",
      "Epoch 76, loss: 0.07813626900315285\n",
      "Test_acc: 0.7\n",
      "--------------------------\n",
      "Epoch 77, loss: 0.07772481068968773\n",
      "Test_acc: 0.7\n",
      "--------------------------\n",
      "Epoch 78, loss: 0.07731806673109531\n",
      "Test_acc: 0.7\n",
      "--------------------------\n",
      "Epoch 79, loss: 0.07691597566008568\n",
      "Test_acc: 0.7\n",
      "--------------------------\n",
      "Epoch 80, loss: 0.07651844993233681\n",
      "Test_acc: 0.7\n",
      "--------------------------\n",
      "Epoch 81, loss: 0.07612543925642967\n",
      "Test_acc: 0.7333333333333333\n",
      "--------------------------\n",
      "Epoch 82, loss: 0.0757368616759777\n",
      "Test_acc: 0.7333333333333333\n",
      "--------------------------\n",
      "Epoch 83, loss: 0.07535265013575554\n",
      "Test_acc: 0.7333333333333333\n",
      "--------------------------\n",
      "Epoch 84, loss: 0.07497274875640869\n",
      "Test_acc: 0.7333333333333333\n",
      "--------------------------\n",
      "Epoch 85, loss: 0.07459708396345377\n",
      "Test_acc: 0.7666666666666667\n",
      "--------------------------\n",
      "Epoch 86, loss: 0.074225596152246\n",
      "Test_acc: 0.7666666666666667\n",
      "--------------------------\n",
      "Epoch 87, loss: 0.07385822664946318\n",
      "Test_acc: 0.7666666666666667\n",
      "--------------------------\n",
      "Epoch 88, loss: 0.07349492143839598\n",
      "Test_acc: 0.7666666666666667\n",
      "--------------------------\n",
      "Epoch 89, loss: 0.07313562091439962\n",
      "Test_acc: 0.7666666666666667\n",
      "--------------------------\n",
      "Epoch 90, loss: 0.07278026547282934\n",
      "Test_acc: 0.7666666666666667\n",
      "--------------------------\n",
      "Epoch 91, loss: 0.0724287973716855\n",
      "Test_acc: 0.7666666666666667\n",
      "--------------------------\n",
      "Epoch 92, loss: 0.07208118494600058\n",
      "Test_acc: 0.7666666666666667\n",
      "--------------------------\n",
      "Epoch 93, loss: 0.07173734344542027\n",
      "Test_acc: 0.8\n",
      "--------------------------\n",
      "Epoch 94, loss: 0.07139724027365446\n",
      "Test_acc: 0.8\n",
      "--------------------------\n",
      "Epoch 95, loss: 0.07106082234531641\n",
      "Test_acc: 0.8\n",
      "--------------------------\n",
      "Epoch 96, loss: 0.07072803378105164\n",
      "Test_acc: 0.8\n",
      "--------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97, loss: 0.0703988391906023\n",
      "Test_acc: 0.8\n",
      "--------------------------\n",
      "Epoch 98, loss: 0.07007317896932364\n",
      "Test_acc: 0.8333333333333334\n",
      "--------------------------\n",
      "Epoch 99, loss: 0.0697510140016675\n",
      "Test_acc: 0.8666666666666667\n",
      "--------------------------\n",
      "Epoch 100, loss: 0.06943229213356972\n",
      "Test_acc: 0.8666666666666667\n",
      "--------------------------\n",
      "Epoch 101, loss: 0.06911696959286928\n",
      "Test_acc: 0.8666666666666667\n",
      "--------------------------\n",
      "Epoch 102, loss: 0.06880500447005033\n",
      "Test_acc: 0.8666666666666667\n",
      "--------------------------\n",
      "Epoch 103, loss: 0.068496348336339\n",
      "Test_acc: 0.8666666666666667\n",
      "--------------------------\n",
      "Epoch 104, loss: 0.06819095648825169\n",
      "Test_acc: 0.8666666666666667\n",
      "--------------------------\n",
      "Epoch 105, loss: 0.06788879167288542\n",
      "Test_acc: 0.8666666666666667\n",
      "--------------------------\n",
      "Epoch 106, loss: 0.06758981943130493\n",
      "Test_acc: 0.8666666666666667\n",
      "--------------------------\n",
      "Epoch 107, loss: 0.0672939820215106\n",
      "Test_acc: 0.9\n",
      "--------------------------\n",
      "Epoch 108, loss: 0.06700124125927687\n",
      "Test_acc: 0.9\n",
      "--------------------------\n",
      "Epoch 109, loss: 0.0667115617543459\n",
      "Test_acc: 0.9\n",
      "--------------------------\n",
      "Epoch 110, loss: 0.06642490904778242\n",
      "Test_acc: 0.9\n",
      "--------------------------\n",
      "Epoch 111, loss: 0.06614123564213514\n",
      "Test_acc: 0.9\n",
      "--------------------------\n",
      "Epoch 112, loss: 0.0658605070784688\n",
      "Test_acc: 0.9\n",
      "--------------------------\n",
      "Epoch 113, loss: 0.06558268051594496\n",
      "Test_acc: 0.9\n",
      "--------------------------\n",
      "Epoch 114, loss: 0.06530772615224123\n",
      "Test_acc: 0.9\n",
      "--------------------------\n",
      "Epoch 115, loss: 0.06503560487180948\n",
      "Test_acc: 0.9\n",
      "--------------------------\n",
      "Epoch 116, loss: 0.06476627103984356\n",
      "Test_acc: 0.9\n",
      "--------------------------\n",
      "Epoch 117, loss: 0.06449970323592424\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 118, loss: 0.06423585396260023\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 119, loss: 0.06397469434887171\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 120, loss: 0.06371618993580341\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 121, loss: 0.06346031092107296\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 122, loss: 0.06320700887590647\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 123, loss: 0.06295627448707819\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 124, loss: 0.0627080425620079\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 125, loss: 0.06246231310069561\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 126, loss: 0.06221904046833515\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 127, loss: 0.061978183686733246\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 128, loss: 0.06173973251134157\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 129, loss: 0.06150364316999912\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 130, loss: 0.061269884929060936\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 131, loss: 0.06103843171149492\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 132, loss: 0.06080925837159157\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 133, loss: 0.06058232951909304\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 134, loss: 0.06035762187093496\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 135, loss: 0.060135108418762684\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 136, loss: 0.05991474911570549\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 137, loss: 0.05969652533531189\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 138, loss: 0.05948041286319494\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 139, loss: 0.05926638375967741\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 140, loss: 0.05905440915375948\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 141, loss: 0.058844463899731636\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 142, loss: 0.05863652843981981\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 143, loss: 0.058430567383766174\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 144, loss: 0.05822655465453863\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 145, loss: 0.05802448093891144\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 146, loss: 0.057824309915304184\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 147, loss: 0.057626024819910526\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 148, loss: 0.0574295949190855\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 149, loss: 0.0572349950671196\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 150, loss: 0.05704221222549677\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 151, loss: 0.05685121566057205\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 152, loss: 0.05666199512779713\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 153, loss: 0.05647451803088188\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 154, loss: 0.056288767606019974\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 155, loss: 0.05610471125692129\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 156, loss: 0.055922345258295536\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 157, loss: 0.0557416332885623\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 158, loss: 0.05556256230920553\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 159, loss: 0.05538512021303177\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 160, loss: 0.05520927347242832\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 161, loss: 0.0550350034609437\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 162, loss: 0.05486230552196503\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 163, loss: 0.05469114240258932\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 164, loss: 0.05452151130884886\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 165, loss: 0.05435337871313095\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 166, loss: 0.054186731576919556\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 167, loss: 0.054021554067730904\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 168, loss: 0.0538578312844038\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 169, loss: 0.053695546463131905\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 170, loss: 0.0535346744582057\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 171, loss: 0.05337520595639944\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 172, loss: 0.05321711394935846\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 173, loss: 0.05306038819253445\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 174, loss: 0.05290501844137907\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 175, loss: 0.05275098513811827\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 176, loss: 0.0525982566177845\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 177, loss: 0.052446840330958366\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 178, loss: 0.05229670833796263\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 179, loss: 0.05214785039424896\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 180, loss: 0.052000245079398155\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 181, loss: 0.05185388680547476\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 182, loss: 0.05170875135809183\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 183, loss: 0.05156483128666878\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 184, loss: 0.051422109827399254\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 185, loss: 0.05128058139234781\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 186, loss: 0.05114021245390177\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 187, loss: 0.051001012325286865\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 188, loss: 0.05086293909698725\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 189, loss: 0.05072600580751896\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 190, loss: 0.050590199418365955\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 191, loss: 0.050455489195883274\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 192, loss: 0.05032187420874834\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 193, loss: 0.05018933489918709\n",
      "Test_acc: 1.0\n",
      "--------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 194, loss: 0.050057861022651196\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 195, loss: 0.04992745537310839\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 196, loss: 0.04979808162897825\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 197, loss: 0.04966974165290594\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 198, loss: 0.04954242613166571\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 199, loss: 0.04941611271351576\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 200, loss: 0.04929080419242382\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 201, loss: 0.049166472628712654\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 202, loss: 0.04904312454164028\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 203, loss: 0.048920731991529465\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 204, loss: 0.04879929404705763\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 205, loss: 0.04867880139499903\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 206, loss: 0.04855923820286989\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 207, loss: 0.048440598882734776\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 208, loss: 0.04832287319004536\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 209, loss: 0.048206047154963017\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 210, loss: 0.04809010960161686\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 211, loss: 0.04797505587339401\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 212, loss: 0.047860877588391304\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 213, loss: 0.04774755984544754\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 214, loss: 0.0476350924000144\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 215, loss: 0.047523465007543564\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 216, loss: 0.0474126823246479\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 217, loss: 0.04730272479355335\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 218, loss: 0.047193579375743866\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 219, loss: 0.04708524886518717\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 220, loss: 0.04697770718485117\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 221, loss: 0.04687096644192934\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 222, loss: 0.04676500428467989\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 223, loss: 0.046659816056489944\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 224, loss: 0.046555389650166035\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 225, loss: 0.04645173158496618\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 226, loss: 0.046348826959729195\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 227, loss: 0.04624664783477783\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 228, loss: 0.04614521563053131\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 229, loss: 0.04604450520128012\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 230, loss: 0.04594451282173395\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 231, loss: 0.04584523383527994\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 232, loss: 0.045746659860014915\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 233, loss: 0.045648783445358276\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 234, loss: 0.045551598072052\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 235, loss: 0.04545509070158005\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 236, loss: 0.045359269715845585\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 237, loss: 0.0452641025185585\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 238, loss: 0.04516960587352514\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 239, loss: 0.04507576394826174\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 240, loss: 0.04498256929218769\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 241, loss: 0.044890016317367554\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 242, loss: 0.044798108749091625\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 243, loss: 0.04470681492239237\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 244, loss: 0.04461614973843098\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 245, loss: 0.04452611040323973\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 246, loss: 0.04443667363375425\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 247, loss: 0.044347841292619705\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 248, loss: 0.04425961058586836\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 249, loss: 0.044171969406306744\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 250, loss: 0.04408490750938654\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 251, loss: 0.04399843979626894\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 252, loss: 0.043912542052567005\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 253, loss: 0.043827205896377563\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 254, loss: 0.043742443434894085\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 255, loss: 0.043658239766955376\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 256, loss: 0.04357459023594856\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 257, loss: 0.043491488322615623\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 258, loss: 0.04340891819447279\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 259, loss: 0.04332689568400383\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 260, loss: 0.043245404958724976\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 261, loss: 0.043164435774087906\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 262, loss: 0.04308399744331837\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 263, loss: 0.043004066683351994\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 264, loss: 0.04292465094476938\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 265, loss: 0.04284574184566736\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 266, loss: 0.04276733845472336\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 267, loss: 0.042689427733421326\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 268, loss: 0.042612007819116116\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 269, loss: 0.042535082437098026\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 270, loss: 0.04245864413678646\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 271, loss: 0.0423826826736331\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 272, loss: 0.04230719245970249\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 273, loss: 0.042232176288962364\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 274, loss: 0.04215762112289667\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 275, loss: 0.042083531618118286\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 276, loss: 0.04200989846140146\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 277, loss: 0.04193671327084303\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 278, loss: 0.04186398349702358\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 279, loss: 0.04179169982671738\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 280, loss: 0.041719856671988964\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 281, loss: 0.04164844285696745\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 282, loss: 0.0415774704888463\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 283, loss: 0.041506923735141754\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 284, loss: 0.041436802595853806\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 285, loss: 0.041367098689079285\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 286, loss: 0.04129782132804394\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 287, loss: 0.04122895281761885\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 288, loss: 0.04116049222648144\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 289, loss: 0.04109244793653488\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 290, loss: 0.04102479945868254\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 291, loss: 0.04095755238085985\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 292, loss: 0.04089069180190563\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 293, loss: 0.04082423448562622\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 294, loss: 0.040758166462183\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 295, loss: 0.040692479349672794\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 296, loss: 0.04062717128545046\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 297, loss: 0.040562248788774014\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 298, loss: 0.040497696958482265\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 299, loss: 0.04043351951986551\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 300, loss: 0.04036971367895603\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 301, loss: 0.04030627105385065\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 302, loss: 0.040243194438517094\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 303, loss: 0.04018046800047159\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 304, loss: 0.04011810338124633\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 305, loss: 0.04005609406158328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 306, loss: 0.039994440507143736\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 307, loss: 0.03993312222883105\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 308, loss: 0.039872155059129\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 309, loss: 0.03981153108179569\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 310, loss: 0.03975124331191182\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 311, loss: 0.03969129594042897\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 312, loss: 0.039631680119782686\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 313, loss: 0.03957238281145692\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 314, loss: 0.03951342590153217\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 315, loss: 0.03945479029789567\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 316, loss: 0.03939648391678929\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 317, loss: 0.03933848859742284\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 318, loss: 0.03928081085905433\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 319, loss: 0.039223446510732174\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 320, loss: 0.03916640114039183\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 321, loss: 0.03910966124385595\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 322, loss: 0.039053221233189106\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 323, loss: 0.038997095078229904\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 324, loss: 0.03894126741215587\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 325, loss: 0.038885737769305706\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 326, loss: 0.03883049916476011\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 327, loss: 0.038775558583438396\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 328, loss: 0.03872091369703412\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 329, loss: 0.038666554260998964\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 330, loss: 0.0386124849319458\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 331, loss: 0.03855870245024562\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 332, loss: 0.03850520076230168\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 333, loss: 0.03845197660848498\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 334, loss: 0.03839903557673097\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 335, loss: 0.03834636835381389\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 336, loss: 0.03829397959634662\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 337, loss: 0.03824185347184539\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 338, loss: 0.03819000208750367\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 339, loss: 0.038138415198773146\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 340, loss: 0.03808710025623441\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 341, loss: 0.03803604422137141\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 342, loss: 0.03798525780439377\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 343, loss: 0.037934715393930674\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 344, loss: 0.03788443887606263\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 345, loss: 0.03783441847190261\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 346, loss: 0.03778464952483773\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 347, loss: 0.03773513389751315\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 348, loss: 0.03768586507067084\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 349, loss: 0.03763684304431081\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 350, loss: 0.03758806874975562\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 351, loss: 0.03753953706473112\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 352, loss: 0.037491250317543745\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 353, loss: 0.037443204782903194\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 354, loss: 0.03739539487287402\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 355, loss: 0.0373478215187788\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 356, loss: 0.03730047307908535\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 357, loss: 0.03725337516516447\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 358, loss: 0.03720649844035506\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 359, loss: 0.037159846629947424\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 360, loss: 0.03711342951282859\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 361, loss: 0.03706724522635341\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 362, loss: 0.03702127141878009\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 363, loss: 0.036975529976189137\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 364, loss: 0.03693000553175807\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 365, loss: 0.0368847013451159\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 366, loss: 0.036839612759649754\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 367, loss: 0.03679474862292409\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 368, loss: 0.03675009310245514\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 369, loss: 0.036705652717500925\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 370, loss: 0.03666142327710986\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 371, loss: 0.036617396865040064\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 372, loss: 0.03657358651980758\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 373, loss: 0.03652997920289636\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 374, loss: 0.036486583296209574\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 375, loss: 0.03644339041784406\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 376, loss: 0.03640039311721921\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 377, loss: 0.0363576035015285\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 378, loss: 0.036315012723207474\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 379, loss: 0.03627262031659484\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 380, loss: 0.036230423022061586\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 381, loss: 0.0361884250305593\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 382, loss: 0.03614661889150739\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 383, loss: 0.03610500367358327\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 384, loss: 0.03606357052922249\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 385, loss: 0.036022343672811985\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 386, loss: 0.035981299821287394\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 387, loss: 0.03594044363126159\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 388, loss: 0.03589977417141199\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 389, loss: 0.035859286319464445\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 390, loss: 0.03581898985430598\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 391, loss: 0.03577886521816254\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 392, loss: 0.03573893057182431\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 393, loss: 0.03569917054846883\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 394, loss: 0.03565958747640252\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 395, loss: 0.0356201883405447\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 396, loss: 0.03558096243068576\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 397, loss: 0.03554190881550312\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 398, loss: 0.03550303215160966\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 399, loss: 0.03546432638540864\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 400, loss: 0.035425789188593626\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 401, loss: 0.03538742894306779\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 402, loss: 0.03534922935068607\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 403, loss: 0.03531120624393225\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 404, loss: 0.035273338202387094\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 405, loss: 0.03523564711213112\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 406, loss: 0.03519811388105154\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 407, loss: 0.03516074409708381\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 408, loss: 0.03512354660779238\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 409, loss: 0.035086494870483875\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 410, loss: 0.03504961961880326\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 411, loss: 0.03501288779079914\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 412, loss: 0.03497632360085845\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 413, loss: 0.03493990935385227\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 414, loss: 0.03490366041660309\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 415, loss: 0.03486755723133683\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 416, loss: 0.034831615164875984\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 417, loss: 0.03479582304134965\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 418, loss: 0.03476017666980624\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 419, loss: 0.034724689088761806\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 420, loss: 0.03468935191631317\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 421, loss: 0.03465416422113776\n",
      "Test_acc: 1.0\n",
      "--------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 422, loss: 0.03461912041530013\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 423, loss: 0.0345842270180583\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 424, loss: 0.03454947331920266\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 425, loss: 0.03451487002894282\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 426, loss: 0.034480408765375614\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 427, loss: 0.03444609511643648\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 428, loss: 0.03441191930323839\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 429, loss: 0.034377886448055506\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 430, loss: 0.0343439974822104\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 431, loss: 0.034310245886445045\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 432, loss: 0.034276632592082024\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 433, loss: 0.03424314968287945\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 434, loss: 0.03420981438830495\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 435, loss: 0.03417661041021347\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 436, loss: 0.03414354659616947\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 437, loss: 0.03411061270162463\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 438, loss: 0.03407781617715955\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 439, loss: 0.03404514491558075\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 440, loss: 0.034012613352388144\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 441, loss: 0.03398020705208182\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 442, loss: 0.03394793486222625\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 443, loss: 0.033915786538273096\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 444, loss: 0.033883772790431976\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 445, loss: 0.03385189129039645\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 446, loss: 0.033820131327956915\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 447, loss: 0.03378849755972624\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 448, loss: 0.03375699184834957\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 449, loss: 0.033725605346262455\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 450, loss: 0.03369434783235192\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 451, loss: 0.03366321325302124\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 452, loss: 0.03363220160827041\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 453, loss: 0.0336013101041317\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 454, loss: 0.03357054525986314\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 455, loss: 0.033539887983351946\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 456, loss: 0.033509367145597935\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 457, loss: 0.03347895201295614\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 458, loss: 0.033448657020926476\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 459, loss: 0.033418480306863785\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 460, loss: 0.03338842652738094\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 461, loss: 0.03335848497226834\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 462, loss: 0.03332865657284856\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 463, loss: 0.033298938535153866\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 464, loss: 0.03326933877542615\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 465, loss: 0.033239856362342834\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 466, loss: 0.03321048244833946\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 467, loss: 0.03318121982738376\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 468, loss: 0.033152075950056314\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 469, loss: 0.03312302986159921\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 470, loss: 0.03309411043301225\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 471, loss: 0.033065286464989185\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 472, loss: 0.03303657751530409\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 473, loss: 0.033007977064698935\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 474, loss: 0.03297947999089956\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 475, loss: 0.03295109001919627\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 476, loss: 0.032922806683927774\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 477, loss: 0.03289463324472308\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 478, loss: 0.03286656038835645\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 479, loss: 0.03283859696239233\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 480, loss: 0.032810729928314686\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 481, loss: 0.03278297185897827\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 482, loss: 0.03275531157851219\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 483, loss: 0.03272775514051318\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 484, loss: 0.03270029602572322\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 485, loss: 0.03267294820398092\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 486, loss: 0.03264569165185094\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 487, loss: 0.03261853335425258\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 488, loss: 0.03259148774668574\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 489, loss: 0.03256453201174736\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 490, loss: 0.032537673600018024\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 491, loss: 0.032510916236788034\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 492, loss: 0.032484252005815506\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 493, loss: 0.032457681372761726\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 494, loss: 0.03243121085688472\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 495, loss: 0.032404832541942596\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 496, loss: 0.032378556206822395\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 497, loss: 0.03235236741602421\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 498, loss: 0.03232627036049962\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 499, loss: 0.032300276681780815\n",
      "Test_acc: 1.0\n",
      "--------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "# 利用鸢尾花数据集，实现前向传播、反向传播，可视化loss曲线\n",
    "\n",
    "# 导入所需模块\n",
    "import tensorflow as tf\n",
    "from sklearn import datasets\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 导入数据，分别为输入特征和标签\n",
    "x_data = datasets.load_iris().data\n",
    "y_data = datasets.load_iris().target\n",
    "\n",
    "# 随机打乱数据（因为原始数据是顺序的，顺序不打乱会影响准确率）\n",
    "# seed: 随机数种子，是一个整数，当设置之后，每次生成的随机数都一样（为方便教学，以保每位同学结果一致）\n",
    "np.random.seed(116)  # 使用相同的seed，保证输入特征和标签一一对应\n",
    "np.random.shuffle(x_data)\n",
    "np.random.seed(116)\n",
    "np.random.shuffle(y_data)\n",
    "tf.random.set_seed(116)\n",
    "\n",
    "# 将打乱后的数据集分割为训练集和测试集，训练集为前120行，测试集为后30行\n",
    "x_train = x_data[:-30]\n",
    "y_train = y_data[:-30]\n",
    "x_test = x_data[-30:]\n",
    "y_test = y_data[-30:]\n",
    "\n",
    "# 转换x的数据类型，否则后面矩阵相乘时会因数据类型不一致报错\n",
    "x_train = tf.cast(x_train, tf.float32)\n",
    "x_test = tf.cast(x_test, tf.float32)\n",
    "\n",
    "# from_tensor_slices函数使输入特征和标签值一一对应。（把数据集分批次，每个批次batch组数据）\n",
    "train_db = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
    "test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)\n",
    "\n",
    "# 生成神经网络的参数，4个输入特征故，输入层为4个输入节点；因为3分类，故输出层为3个神经元\n",
    "# 用tf.Variable()标记参数可训练\n",
    "# 使用seed使每次生成的随机数相同（方便教学，使大家结果都一致，在现实使用时不写seed）\n",
    "w1 = tf.Variable(tf.random.truncated_normal([4, 3], stddev=0.1, seed=1))\n",
    "b1 = tf.Variable(tf.random.truncated_normal([3], stddev=0.1, seed=1))\n",
    "\n",
    "lr = 0.1  # 学习率为0.1\n",
    "train_loss_results = []  # 将每轮的loss记录在此列表中，为后续画loss曲线提供数据\n",
    "test_acc = []  # 将每轮的acc记录在此列表中，为后续画acc曲线提供数据\n",
    "epoch = 500  # 循环500轮\n",
    "loss_all = 0  # 每轮分4个step，loss_all记录四个step生成的4个loss的和\n",
    "\n",
    "# 训练部分\n",
    "for epoch in range(epoch):  #数据集级别的循环，每个epoch循环一次数据集\n",
    "    for step, (x_train, y_train) in enumerate(train_db):  #batch级别的循环 ，每个step循环一个batch\n",
    "        with tf.GradientTape() as tape:  # with结构记录梯度信息\n",
    "            y = tf.matmul(x_train, w1) + b1  # 神经网络乘加运算\n",
    "            y = tf.nn.softmax(y)  # 使输出y符合概率分布（此操作后与独热码同量级，可相减求loss）\n",
    "            y_ = tf.one_hot(y_train, depth=3)  # 将标签值转换为独热码格式，方便计算loss和accuracy\n",
    "            loss = tf.reduce_mean(tf.square(y_ - y))  # 采用均方误差损失函数mse = mean(sum(y-out)^2)\n",
    "            loss_all += loss.numpy()  # 将每个step计算出的loss累加，为后续求loss平均值提供数据，这样计算的loss更准确\n",
    "        # 计算loss对各个参数的梯度\n",
    "        grads = tape.gradient(loss, [w1, b1])\n",
    "\n",
    "        # 实现梯度更新 w1 = w1 - lr * w1_grad    b = b - lr * b_grad\n",
    "        w1.assign_sub(lr * grads[0])  # 参数w1自更新\n",
    "        b1.assign_sub(lr * grads[1])  # 参数b自更新\n",
    "\n",
    "    # 每个epoch，打印loss信息\n",
    "    print(\"Epoch {}, loss: {}\".format(epoch, loss_all/4))\n",
    "    train_loss_results.append(loss_all / 4)  # 将4个step的loss求平均记录在此变量中\n",
    "    loss_all = 0  # loss_all归零，为记录下一个epoch的loss做准备\n",
    "\n",
    "    # 测试部分\n",
    "    # total_correct为预测对的样本个数, total_number为测试的总样本数，将这两个变量都初始化为0\n",
    "    total_correct, total_number = 0, 0\n",
    "    for x_test, y_test in test_db:\n",
    "        # 使用更新后的参数进行预测\n",
    "        y = tf.matmul(x_test, w1) + b1\n",
    "        y = tf.nn.softmax(y)\n",
    "        pred = tf.argmax(y, axis=1)  # 返回y中最大值的索引，即预测的分类\n",
    "        # 将pred转换为y_test的数据类型\n",
    "        pred = tf.cast(pred, dtype=y_test.dtype)\n",
    "        # 若分类正确，则correct=1，否则为0，将bool型的结果转换为int型\n",
    "        correct = tf.cast(tf.equal(pred, y_test), dtype=tf.int32)\n",
    "        # 将每个batch的correct数加起来\n",
    "        correct = tf.reduce_sum(correct)\n",
    "        # 将所有batch中的correct数加起来\n",
    "        total_correct += int(correct)\n",
    "        # total_number为测试的总样本数，也就是x_test的行数，shape[0]返回变量的行数\n",
    "        total_number += x_test.shape[0]\n",
    "    # 总的准确率等于total_correct/total_number\n",
    "    acc = total_correct / total_number\n",
    "    test_acc.append(acc)\n",
    "    print(\"Test_acc:\", acc)\n",
    "    print(\"--------------------------\")\n",
    "\n",
    "# 绘制 loss 曲线\n",
    "plt.title('Loss Function Curve')  # 图片标题\n",
    "plt.xlabel('Epoch')  # x轴变量名称\n",
    "plt.ylabel('Loss')  # y轴变量名称\n",
    "plt.plot(train_loss_results, label=\"$Loss$\")  # 逐点画出trian_loss_results值并连线，连线图标是Loss\n",
    "plt.legend()  # 画出曲线图标\n",
    "plt.show()  # 画出图像\n",
    "\n",
    "# 绘制 Accuracy 曲线\n",
    "plt.title('Acc Curve')  # 图片标题\n",
    "plt.xlabel('Epoch')  # x轴变量名称\n",
    "plt.ylabel('Acc')  # y轴变量名称\n",
    "plt.plot(test_acc, label=\"$Accuracy$\")  # 逐点画出test_acc值并连线，连线图标是Accuracy\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 鸢尾花分类用keras六步法实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 120 samples, validate on 30 samples\n",
      "Epoch 1/500\n",
      "120/120 [==============================] - 0s 3ms/sample - loss: 2.1962 - sparse_categorical_accuracy: 0.3500\n",
      "Epoch 2/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.8823 - sparse_categorical_accuracy: 0.6417\n",
      "Epoch 3/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 1.0519 - sparse_categorical_accuracy: 0.6500\n",
      "Epoch 4/500\n",
      "120/120 [==============================] - 0s 78us/sample - loss: 0.8097 - sparse_categorical_accuracy: 0.6333\n",
      "Epoch 5/500\n",
      "120/120 [==============================] - 0s 80us/sample - loss: 1.0982 - sparse_categorical_accuracy: 0.6250\n",
      "Epoch 6/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.6229 - sparse_categorical_accuracy: 0.7167\n",
      "Epoch 7/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.9548 - sparse_categorical_accuracy: 0.6333\n",
      "Epoch 8/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.5668 - sparse_categorical_accuracy: 0.7417\n",
      "Epoch 9/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.6024 - sparse_categorical_accuracy: 0.7167\n",
      "Epoch 10/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.5653 - sparse_categorical_accuracy: 0.7750\n",
      "Epoch 11/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.5925 - sparse_categorical_accuracy: 0.7250\n",
      "Epoch 12/500\n",
      "120/120 [==============================] - 0s 78us/sample - loss: 0.6456 - sparse_categorical_accuracy: 0.6667\n",
      "Epoch 13/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.5960 - sparse_categorical_accuracy: 0.7250\n",
      "Epoch 14/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.6188 - sparse_categorical_accuracy: 0.6750\n",
      "Epoch 15/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.5207 - sparse_categorical_accuracy: 0.8417\n",
      "Epoch 16/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.5884 - sparse_categorical_accuracy: 0.6833\n",
      "Epoch 17/500\n",
      "120/120 [==============================] - 0s 74us/sample - loss: 0.4988 - sparse_categorical_accuracy: 0.8083\n",
      "Epoch 18/500\n",
      "120/120 [==============================] - 0s 74us/sample - loss: 0.5115 - sparse_categorical_accuracy: 0.7583\n",
      "Epoch 19/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.6111 - sparse_categorical_accuracy: 0.6917\n",
      "Epoch 20/500\n",
      "120/120 [==============================] - 0s 1ms/sample - loss: 0.7344 - sparse_categorical_accuracy: 0.6833 - val_loss: 0.6025 - val_sparse_categorical_accuracy: 0.5333\n",
      "Epoch 21/500\n",
      "120/120 [==============================] - 0s 92us/sample - loss: 0.4730 - sparse_categorical_accuracy: 0.8000\n",
      "Epoch 22/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.6484 - sparse_categorical_accuracy: 0.6750\n",
      "Epoch 23/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.5528 - sparse_categorical_accuracy: 0.7917\n",
      "Epoch 24/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.7129 - sparse_categorical_accuracy: 0.6500\n",
      "Epoch 25/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.5139 - sparse_categorical_accuracy: 0.7667\n",
      "Epoch 26/500\n",
      "120/120 [==============================] - 0s 78us/sample - loss: 0.4982 - sparse_categorical_accuracy: 0.8000\n",
      "Epoch 27/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 1.1382 - sparse_categorical_accuracy: 0.5917\n",
      "Epoch 28/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.5493 - sparse_categorical_accuracy: 0.7000\n",
      "Epoch 29/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.6210 - sparse_categorical_accuracy: 0.7000\n",
      "Epoch 30/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.5422 - sparse_categorical_accuracy: 0.7000\n",
      "Epoch 31/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.4212 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 32/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.5288 - sparse_categorical_accuracy: 0.7000\n",
      "Epoch 33/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.6907 - sparse_categorical_accuracy: 0.6583\n",
      "Epoch 34/500\n",
      "120/120 [==============================] - 0s 74us/sample - loss: 0.4772 - sparse_categorical_accuracy: 0.8083\n",
      "Epoch 35/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.4436 - sparse_categorical_accuracy: 0.8750\n",
      "Epoch 36/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.5897 - sparse_categorical_accuracy: 0.7417\n",
      "Epoch 37/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.4435 - sparse_categorical_accuracy: 0.8250\n",
      "Epoch 38/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.4061 - sparse_categorical_accuracy: 0.8833\n",
      "Epoch 39/500\n",
      "120/120 [==============================] - 0s 87us/sample - loss: 0.4785 - sparse_categorical_accuracy: 0.7583\n",
      "Epoch 40/500\n",
      "120/120 [==============================] - 0s 122us/sample - loss: 0.4328 - sparse_categorical_accuracy: 0.8417 - val_loss: 0.5688 - val_sparse_categorical_accuracy: 0.6000\n",
      "Epoch 41/500\n",
      "120/120 [==============================] - 0s 87us/sample - loss: 0.4749 - sparse_categorical_accuracy: 0.7917\n",
      "Epoch 42/500\n",
      "120/120 [==============================] - 0s 90us/sample - loss: 0.5713 - sparse_categorical_accuracy: 0.7167\n",
      "Epoch 43/500\n",
      "120/120 [==============================] - 0s 91us/sample - loss: 0.5949 - sparse_categorical_accuracy: 0.7000\n",
      "Epoch 44/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3894 - sparse_categorical_accuracy: 0.8750\n",
      "Epoch 45/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.6079 - sparse_categorical_accuracy: 0.7333\n",
      "Epoch 46/500\n",
      "120/120 [==============================] - 0s 74us/sample - loss: 0.6114 - sparse_categorical_accuracy: 0.6917\n",
      "Epoch 47/500\n",
      "120/120 [==============================] - 0s 74us/sample - loss: 0.4088 - sparse_categorical_accuracy: 0.8833\n",
      "Epoch 48/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.4352 - sparse_categorical_accuracy: 0.8750\n",
      "Epoch 49/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.5262 - sparse_categorical_accuracy: 0.7417\n",
      "Epoch 50/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.4200 - sparse_categorical_accuracy: 0.8917\n",
      "Epoch 51/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.6255 - sparse_categorical_accuracy: 0.6583\n",
      "Epoch 52/500\n",
      "120/120 [==============================] - 0s 79us/sample - loss: 0.8264 - sparse_categorical_accuracy: 0.6833\n",
      "Epoch 53/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.4714 - sparse_categorical_accuracy: 0.8250\n",
      "Epoch 54/500\n",
      "120/120 [==============================] - 0s 78us/sample - loss: 0.5078 - sparse_categorical_accuracy: 0.7500\n",
      "Epoch 55/500\n",
      "120/120 [==============================] - 0s 74us/sample - loss: 0.4240 - sparse_categorical_accuracy: 0.8750\n",
      "Epoch 56/500\n",
      "120/120 [==============================] - 0s 88us/sample - loss: 0.4657 - sparse_categorical_accuracy: 0.8583\n",
      "Epoch 57/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3779 - sparse_categorical_accuracy: 0.9167\n",
      "Epoch 58/500\n",
      "120/120 [==============================] - 0s 92us/sample - loss: 0.3928 - sparse_categorical_accuracy: 0.9083\n",
      "Epoch 59/500\n",
      "120/120 [==============================] - 0s 89us/sample - loss: 0.3994 - sparse_categorical_accuracy: 0.8917\n",
      "Epoch 60/500\n",
      "120/120 [==============================] - 0s 137us/sample - loss: 0.4248 - sparse_categorical_accuracy: 0.8667 - val_loss: 0.9493 - val_sparse_categorical_accuracy: 0.5333\n",
      "Epoch 61/500\n",
      "120/120 [==============================] - 0s 74us/sample - loss: 0.7443 - sparse_categorical_accuracy: 0.6833\n",
      "Epoch 62/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.4022 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 63/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.4407 - sparse_categorical_accuracy: 0.8500\n",
      "Epoch 64/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.4254 - sparse_categorical_accuracy: 0.8583\n",
      "Epoch 65/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 82us/sample - loss: 0.4211 - sparse_categorical_accuracy: 0.8667\n",
      "Epoch 66/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3679 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 67/500\n",
      "120/120 [==============================] - 0s 81us/sample - loss: 0.4784 - sparse_categorical_accuracy: 0.7833\n",
      "Epoch 68/500\n",
      "120/120 [==============================] - 0s 79us/sample - loss: 0.6290 - sparse_categorical_accuracy: 0.6750\n",
      "Epoch 69/500\n",
      "120/120 [==============================] - 0s 79us/sample - loss: 0.3813 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 70/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.3755 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 71/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3886 - sparse_categorical_accuracy: 0.9167\n",
      "Epoch 72/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.4446 - sparse_categorical_accuracy: 0.8167\n",
      "Epoch 73/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3933 - sparse_categorical_accuracy: 0.9333\n",
      "Epoch 74/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.4546 - sparse_categorical_accuracy: 0.8583\n",
      "Epoch 75/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3730 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 76/500\n",
      "120/120 [==============================] - 0s 96us/sample - loss: 0.3745 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 77/500\n",
      "120/120 [==============================] - 0s 94us/sample - loss: 0.3885 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 78/500\n",
      "120/120 [==============================] - 0s 82us/sample - loss: 0.6013 - sparse_categorical_accuracy: 0.7083\n",
      "Epoch 79/500\n",
      "120/120 [==============================] - 0s 92us/sample - loss: 0.3682 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 80/500\n",
      "120/120 [==============================] - 0s 135us/sample - loss: 0.3648 - sparse_categorical_accuracy: 0.9333 - val_loss: 0.3486 - val_sparse_categorical_accuracy: 0.9667\n",
      "Epoch 81/500\n",
      "120/120 [==============================] - 0s 86us/sample - loss: 0.5549 - sparse_categorical_accuracy: 0.7333\n",
      "Epoch 82/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.4028 - sparse_categorical_accuracy: 0.9167\n",
      "Epoch 83/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.3798 - sparse_categorical_accuracy: 0.9083\n",
      "Epoch 84/500\n",
      "120/120 [==============================] - 0s 90us/sample - loss: 0.3935 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 85/500\n",
      "120/120 [==============================] - 0s 79us/sample - loss: 0.4358 - sparse_categorical_accuracy: 0.8500\n",
      "Epoch 86/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.4759 - sparse_categorical_accuracy: 0.8250\n",
      "Epoch 87/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.4273 - sparse_categorical_accuracy: 0.8917\n",
      "Epoch 88/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.5257 - sparse_categorical_accuracy: 0.7500\n",
      "Epoch 89/500\n",
      "120/120 [==============================] - 0s 74us/sample - loss: 0.5146 - sparse_categorical_accuracy: 0.8000\n",
      "Epoch 90/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.4159 - sparse_categorical_accuracy: 0.9083\n",
      "Epoch 91/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.4119 - sparse_categorical_accuracy: 0.8667\n",
      "Epoch 92/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.3800 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 93/500\n",
      "120/120 [==============================] - 0s 89us/sample - loss: 0.4675 - sparse_categorical_accuracy: 0.7917\n",
      "Epoch 94/500\n",
      "120/120 [==============================] - 0s 78us/sample - loss: 0.4614 - sparse_categorical_accuracy: 0.8167\n",
      "Epoch 95/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.3742 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 96/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.3666 - sparse_categorical_accuracy: 0.9333\n",
      "Epoch 97/500\n",
      "120/120 [==============================] - 0s 87us/sample - loss: 0.3699 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 98/500\n",
      "120/120 [==============================] - 0s 88us/sample - loss: 0.3871 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 99/500\n",
      "120/120 [==============================] - 0s 88us/sample - loss: 0.3991 - sparse_categorical_accuracy: 0.9083\n",
      "Epoch 100/500\n",
      "120/120 [==============================] - 0s 123us/sample - loss: 0.3707 - sparse_categorical_accuracy: 0.9333 - val_loss: 0.3488 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 101/500\n",
      "120/120 [==============================] - 0s 74us/sample - loss: 0.3660 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 102/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3743 - sparse_categorical_accuracy: 0.9167\n",
      "Epoch 103/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.4286 - sparse_categorical_accuracy: 0.8500\n",
      "Epoch 104/500\n",
      "120/120 [==============================] - 0s 74us/sample - loss: 0.4726 - sparse_categorical_accuracy: 0.8083\n",
      "Epoch 105/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3971 - sparse_categorical_accuracy: 0.9083\n",
      "Epoch 106/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3626 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 107/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3853 - sparse_categorical_accuracy: 0.8917\n",
      "Epoch 108/500\n",
      "120/120 [==============================] - 0s 74us/sample - loss: 0.4205 - sparse_categorical_accuracy: 0.8750\n",
      "Epoch 109/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.4696 - sparse_categorical_accuracy: 0.8333\n",
      "Epoch 110/500\n",
      "120/120 [==============================] - 0s 88us/sample - loss: 0.4009 - sparse_categorical_accuracy: 0.8917\n",
      "Epoch 111/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.4237 - sparse_categorical_accuracy: 0.8667\n",
      "Epoch 112/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3955 - sparse_categorical_accuracy: 0.9167\n",
      "Epoch 113/500\n",
      "120/120 [==============================] - 0s 87us/sample - loss: 0.3932 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 114/500\n",
      "120/120 [==============================] - 0s 88us/sample - loss: 0.3771 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 115/500\n",
      "120/120 [==============================] - 0s 90us/sample - loss: 0.4146 - sparse_categorical_accuracy: 0.8917\n",
      "Epoch 116/500\n",
      "120/120 [==============================] - 0s 88us/sample - loss: 0.3639 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 117/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.3559 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 118/500\n",
      "120/120 [==============================] - 0s 89us/sample - loss: 0.4085 - sparse_categorical_accuracy: 0.8833\n",
      "Epoch 119/500\n",
      "120/120 [==============================] - 0s 79us/sample - loss: 0.4038 - sparse_categorical_accuracy: 0.8833\n",
      "Epoch 120/500\n",
      "120/120 [==============================] - 0s 124us/sample - loss: 0.3898 - sparse_categorical_accuracy: 0.9083 - val_loss: 0.3566 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 121/500\n",
      "120/120 [==============================] - 0s 74us/sample - loss: 0.3968 - sparse_categorical_accuracy: 0.9083\n",
      "Epoch 122/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.5416 - sparse_categorical_accuracy: 0.8083\n",
      "Epoch 123/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.4635 - sparse_categorical_accuracy: 0.8500\n",
      "Epoch 124/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.3970 - sparse_categorical_accuracy: 0.9000\n",
      "Epoch 125/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.4089 - sparse_categorical_accuracy: 0.8667\n",
      "Epoch 126/500\n",
      "120/120 [==============================] - 0s 78us/sample - loss: 0.3783 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 127/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.4521 - sparse_categorical_accuracy: 0.8667\n",
      "Epoch 128/500\n",
      "120/120 [==============================] - 0s 79us/sample - loss: 0.3612 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 129/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3534 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 130/500\n",
      "120/120 [==============================] - 0s 78us/sample - loss: 0.3624 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 131/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3767 - sparse_categorical_accuracy: 0.9083\n",
      "Epoch 132/500\n",
      "120/120 [==============================] - 0s 90us/sample - loss: 0.4185 - sparse_categorical_accuracy: 0.8750\n",
      "Epoch 133/500\n",
      "120/120 [==============================] - 0s 88us/sample - loss: 0.3814 - sparse_categorical_accuracy: 0.9167\n",
      "Epoch 134/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.4230 - sparse_categorical_accuracy: 0.8917\n",
      "Epoch 135/500\n",
      "120/120 [==============================] - 0s 92us/sample - loss: 0.3636 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 136/500\n",
      "120/120 [==============================] - 0s 78us/sample - loss: 0.3685 - sparse_categorical_accuracy: 0.9333\n",
      "Epoch 137/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3573 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 138/500\n",
      "120/120 [==============================] - 0s 90us/sample - loss: 0.3876 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 139/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3797 - sparse_categorical_accuracy: 0.9167\n",
      "Epoch 140/500\n",
      "120/120 [==============================] - 0s 124us/sample - loss: 0.3610 - sparse_categorical_accuracy: 0.9167 - val_loss: 0.3956 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 141/500\n",
      "120/120 [==============================] - 0s 79us/sample - loss: 0.3889 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 142/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3711 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 143/500\n",
      "120/120 [==============================] - 0s 78us/sample - loss: 0.3936 - sparse_categorical_accuracy: 0.9000\n",
      "Epoch 144/500\n",
      "120/120 [==============================] - 0s 79us/sample - loss: 0.4222 - sparse_categorical_accuracy: 0.8833\n",
      "Epoch 145/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3865 - sparse_categorical_accuracy: 0.9083\n",
      "Epoch 146/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3884 - sparse_categorical_accuracy: 0.9000\n",
      "Epoch 147/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3573 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 148/500\n",
      "120/120 [==============================] - 0s 137us/sample - loss: 0.3567 - sparse_categorical_accuracy: 0.9333\n",
      "Epoch 149/500\n",
      "120/120 [==============================] - 0s 78us/sample - loss: 0.3535 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 150/500\n",
      "120/120 [==============================] - 0s 84us/sample - loss: 0.3700 - sparse_categorical_accuracy: 0.9083\n",
      "Epoch 151/500\n",
      "120/120 [==============================] - 0s 91us/sample - loss: 0.3543 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 152/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3525 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 153/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3923 - sparse_categorical_accuracy: 0.9083\n",
      "Epoch 154/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3644 - sparse_categorical_accuracy: 0.9333\n",
      "Epoch 155/500\n",
      "120/120 [==============================] - 0s 88us/sample - loss: 0.3598 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 156/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3713 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 157/500\n",
      "120/120 [==============================] - 0s 74us/sample - loss: 0.3599 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 158/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3831 - sparse_categorical_accuracy: 0.9333\n",
      "Epoch 159/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.4198 - sparse_categorical_accuracy: 0.8917\n",
      "Epoch 160/500\n",
      "120/120 [==============================] - 0s 123us/sample - loss: 0.3753 - sparse_categorical_accuracy: 0.9083 - val_loss: 0.3352 - val_sparse_categorical_accuracy: 0.9667\n",
      "Epoch 161/500\n",
      "120/120 [==============================] - 0s 88us/sample - loss: 0.3496 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 162/500\n",
      "120/120 [==============================] - 0s 78us/sample - loss: 0.3552 - sparse_categorical_accuracy: 0.9333\n",
      "Epoch 163/500\n",
      "120/120 [==============================] - 0s 79us/sample - loss: 0.3585 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 164/500\n",
      "120/120 [==============================] - 0s 93us/sample - loss: 0.3939 - sparse_categorical_accuracy: 0.8917\n",
      "Epoch 165/500\n",
      "120/120 [==============================] - 0s 86us/sample - loss: 0.3553 - sparse_categorical_accuracy: 0.9750\n",
      "Epoch 166/500\n",
      "120/120 [==============================] - 0s 81us/sample - loss: 0.4774 - sparse_categorical_accuracy: 0.8167\n",
      "Epoch 167/500\n",
      "120/120 [==============================] - 0s 82us/sample - loss: 0.3585 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 168/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.3671 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 169/500\n",
      "120/120 [==============================] - 0s 78us/sample - loss: 0.3551 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 170/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3679 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 171/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3501 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 172/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3583 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 173/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3850 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 174/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3711 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 175/500\n",
      "120/120 [==============================] - 0s 80us/sample - loss: 0.5982 - sparse_categorical_accuracy: 0.7583\n",
      "Epoch 176/500\n",
      "120/120 [==============================] - 0s 79us/sample - loss: 0.4060 - sparse_categorical_accuracy: 0.8750\n",
      "Epoch 177/500\n",
      "120/120 [==============================] - 0s 79us/sample - loss: 0.3808 - sparse_categorical_accuracy: 0.9083\n",
      "Epoch 178/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.3528 - sparse_categorical_accuracy: 0.9333\n",
      "Epoch 179/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3529 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 180/500\n",
      "120/120 [==============================] - 0s 122us/sample - loss: 0.5415 - sparse_categorical_accuracy: 0.8000 - val_loss: 0.3710 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 181/500\n",
      "120/120 [==============================] - 0s 73us/sample - loss: 0.3991 - sparse_categorical_accuracy: 0.8833\n",
      "Epoch 182/500\n",
      "120/120 [==============================] - 0s 88us/sample - loss: 0.3712 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 183/500\n",
      "120/120 [==============================] - 0s 89us/sample - loss: 0.4376 - sparse_categorical_accuracy: 0.8833\n",
      "Epoch 184/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.3740 - sparse_categorical_accuracy: 0.9333\n",
      "Epoch 185/500\n",
      "120/120 [==============================] - 0s 89us/sample - loss: 0.3531 - sparse_categorical_accuracy: 0.9750\n",
      "Epoch 186/500\n",
      "120/120 [==============================] - 0s 78us/sample - loss: 0.4468 - sparse_categorical_accuracy: 0.8667\n",
      "Epoch 187/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3520 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 188/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3960 - sparse_categorical_accuracy: 0.9083\n",
      "Epoch 189/500\n",
      "120/120 [==============================] - 0s 74us/sample - loss: 0.3752 - sparse_categorical_accuracy: 0.9083\n",
      "Epoch 190/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3450 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 191/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3676 - sparse_categorical_accuracy: 0.9083\n",
      "Epoch 192/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.3965 - sparse_categorical_accuracy: 0.8917\n",
      "Epoch 193/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 91us/sample - loss: 0.4040 - sparse_categorical_accuracy: 0.8917\n",
      "Epoch 194/500\n",
      "120/120 [==============================] - 0s 78us/sample - loss: 0.3535 - sparse_categorical_accuracy: 0.9333\n",
      "Epoch 195/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.3798 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 196/500\n",
      "120/120 [==============================] - 0s 74us/sample - loss: 0.3599 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 197/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3632 - sparse_categorical_accuracy: 0.9333\n",
      "Epoch 198/500\n",
      "120/120 [==============================] - 0s 88us/sample - loss: 0.3789 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 199/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3995 - sparse_categorical_accuracy: 0.8750\n",
      "Epoch 200/500\n",
      "120/120 [==============================] - 0s 141us/sample - loss: 0.3566 - sparse_categorical_accuracy: 0.9333 - val_loss: 0.3405 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 201/500\n",
      "120/120 [==============================] - 0s 82us/sample - loss: 0.3695 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 202/500\n",
      "120/120 [==============================] - 0s 78us/sample - loss: 0.3705 - sparse_categorical_accuracy: 0.9083\n",
      "Epoch 203/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.3659 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 204/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3444 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 205/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3482 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 206/500\n",
      "120/120 [==============================] - 0s 78us/sample - loss: 0.3541 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 207/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3461 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 208/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.4319 - sparse_categorical_accuracy: 0.8667\n",
      "Epoch 209/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3794 - sparse_categorical_accuracy: 0.9083\n",
      "Epoch 210/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.4089 - sparse_categorical_accuracy: 0.8667\n",
      "Epoch 211/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.4438 - sparse_categorical_accuracy: 0.8417\n",
      "Epoch 212/500\n",
      "120/120 [==============================] - 0s 74us/sample - loss: 0.3817 - sparse_categorical_accuracy: 0.9167\n",
      "Epoch 213/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3611 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 214/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3471 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 215/500\n",
      "120/120 [==============================] - 0s 87us/sample - loss: 0.3721 - sparse_categorical_accuracy: 0.9167\n",
      "Epoch 216/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3789 - sparse_categorical_accuracy: 0.9167\n",
      "Epoch 217/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3548 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 218/500\n",
      "120/120 [==============================] - 0s 79us/sample - loss: 0.3957 - sparse_categorical_accuracy: 0.8917\n",
      "Epoch 219/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3922 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 220/500\n",
      "120/120 [==============================] - 0s 122us/sample - loss: 0.4800 - sparse_categorical_accuracy: 0.8250 - val_loss: 0.3715 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 221/500\n",
      "120/120 [==============================] - 0s 86us/sample - loss: 0.3449 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 222/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3515 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 223/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3992 - sparse_categorical_accuracy: 0.9083\n",
      "Epoch 224/500\n",
      "120/120 [==============================] - 0s 79us/sample - loss: 0.4384 - sparse_categorical_accuracy: 0.8750\n",
      "Epoch 225/500\n",
      "120/120 [==============================] - 0s 93us/sample - loss: 0.4206 - sparse_categorical_accuracy: 0.9083\n",
      "Epoch 226/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.3515 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 227/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3643 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 228/500\n",
      "120/120 [==============================] - 0s 78us/sample - loss: 0.3677 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 229/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3581 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 230/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3595 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 231/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.4021 - sparse_categorical_accuracy: 0.8917\n",
      "Epoch 232/500\n",
      "120/120 [==============================] - 0s 93us/sample - loss: 0.3488 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 233/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.4141 - sparse_categorical_accuracy: 0.8667\n",
      "Epoch 234/500\n",
      "120/120 [==============================] - 0s 80us/sample - loss: 0.3510 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 235/500\n",
      "120/120 [==============================] - 0s 78us/sample - loss: 0.3453 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 236/500\n",
      "120/120 [==============================] - 0s 78us/sample - loss: 0.3707 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 237/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3780 - sparse_categorical_accuracy: 0.9167\n",
      "Epoch 238/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3629 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 239/500\n",
      "120/120 [==============================] - 0s 89us/sample - loss: 0.3612 - sparse_categorical_accuracy: 0.9333\n",
      "Epoch 240/500\n",
      "120/120 [==============================] - 0s 122us/sample - loss: 0.3823 - sparse_categorical_accuracy: 0.9333 - val_loss: 0.3478 - val_sparse_categorical_accuracy: 0.9000\n",
      "Epoch 241/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3899 - sparse_categorical_accuracy: 0.8917\n",
      "Epoch 242/500\n",
      "120/120 [==============================] - 0s 89us/sample - loss: 0.4695 - sparse_categorical_accuracy: 0.8167\n",
      "Epoch 243/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3549 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 244/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.4236 - sparse_categorical_accuracy: 0.8667\n",
      "Epoch 245/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.4944 - sparse_categorical_accuracy: 0.8583\n",
      "Epoch 246/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3543 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 247/500\n",
      "120/120 [==============================] - 0s 73us/sample - loss: 0.3524 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 248/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3801 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 249/500\n",
      "120/120 [==============================] - 0s 89us/sample - loss: 0.3485 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 250/500\n",
      "120/120 [==============================] - 0s 92us/sample - loss: 0.3434 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 251/500\n",
      "120/120 [==============================] - 0s 80us/sample - loss: 0.3485 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 252/500\n",
      "120/120 [==============================] - 0s 82us/sample - loss: 0.3454 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 253/500\n",
      "120/120 [==============================] - 0s 79us/sample - loss: 0.3492 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 254/500\n",
      "120/120 [==============================] - 0s 78us/sample - loss: 0.3815 - sparse_categorical_accuracy: 0.8917\n",
      "Epoch 255/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.4077 - sparse_categorical_accuracy: 0.8833\n",
      "Epoch 256/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.3796 - sparse_categorical_accuracy: 0.9167\n",
      "Epoch 257/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3603 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 258/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.3593 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 259/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.3550 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 260/500\n",
      "120/120 [==============================] - 0s 137us/sample - loss: 0.3702 - sparse_categorical_accuracy: 0.9333 - val_loss: 0.3728 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 261/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3443 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 262/500\n",
      "120/120 [==============================] - 0s 78us/sample - loss: 0.3642 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 263/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.3592 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 264/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3484 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 265/500\n",
      "120/120 [==============================] - 0s 74us/sample - loss: 0.3374 - sparse_categorical_accuracy: 0.9333\n",
      "Epoch 266/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3543 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 267/500\n",
      "120/120 [==============================] - 0s 89us/sample - loss: 0.3952 - sparse_categorical_accuracy: 0.9333\n",
      "Epoch 268/500\n",
      "120/120 [==============================] - 0s 78us/sample - loss: 0.3469 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 269/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.4562 - sparse_categorical_accuracy: 0.8417\n",
      "Epoch 270/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.3673 - sparse_categorical_accuracy: 0.9083\n",
      "Epoch 271/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3751 - sparse_categorical_accuracy: 0.9167\n",
      "Epoch 272/500\n",
      "120/120 [==============================] - 0s 87us/sample - loss: 0.4066 - sparse_categorical_accuracy: 0.9083\n",
      "Epoch 273/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3520 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 274/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3508 - sparse_categorical_accuracy: 0.9750\n",
      "Epoch 275/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.4247 - sparse_categorical_accuracy: 0.8500\n",
      "Epoch 276/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.5184 - sparse_categorical_accuracy: 0.8000\n",
      "Epoch 277/500\n",
      "120/120 [==============================] - 0s 89us/sample - loss: 0.3586 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 278/500\n",
      "120/120 [==============================] - 0s 90us/sample - loss: 0.4024 - sparse_categorical_accuracy: 0.8917\n",
      "Epoch 279/500\n",
      "120/120 [==============================] - 0s 90us/sample - loss: 0.3472 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 280/500\n",
      "120/120 [==============================] - 0s 123us/sample - loss: 0.3479 - sparse_categorical_accuracy: 0.9417 - val_loss: 0.5195 - val_sparse_categorical_accuracy: 0.6667\n",
      "Epoch 281/500\n",
      "120/120 [==============================] - 0s 78us/sample - loss: 0.5396 - sparse_categorical_accuracy: 0.7750\n",
      "Epoch 282/500\n",
      "120/120 [==============================] - 0s 78us/sample - loss: 0.3576 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 283/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.4066 - sparse_categorical_accuracy: 0.9083\n",
      "Epoch 284/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.3588 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 285/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.4913 - sparse_categorical_accuracy: 0.8417\n",
      "Epoch 286/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.3891 - sparse_categorical_accuracy: 0.8833\n",
      "Epoch 287/500\n",
      "120/120 [==============================] - 0s 78us/sample - loss: 0.3614 - sparse_categorical_accuracy: 0.9750\n",
      "Epoch 288/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3402 - sparse_categorical_accuracy: 0.9833\n",
      "Epoch 289/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3814 - sparse_categorical_accuracy: 0.9167\n",
      "Epoch 290/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3901 - sparse_categorical_accuracy: 0.9000\n",
      "Epoch 291/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3999 - sparse_categorical_accuracy: 0.9000\n",
      "Epoch 292/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3704 - sparse_categorical_accuracy: 0.9333\n",
      "Epoch 293/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3493 - sparse_categorical_accuracy: 0.9333\n",
      "Epoch 294/500\n",
      "120/120 [==============================] - 0s 91us/sample - loss: 0.3948 - sparse_categorical_accuracy: 0.9333\n",
      "Epoch 295/500\n",
      "120/120 [==============================] - 0s 91us/sample - loss: 0.4169 - sparse_categorical_accuracy: 0.8667\n",
      "Epoch 296/500\n",
      "120/120 [==============================] - 0s 88us/sample - loss: 0.3815 - sparse_categorical_accuracy: 0.9333\n",
      "Epoch 297/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.3795 - sparse_categorical_accuracy: 0.9083\n",
      "Epoch 298/500\n",
      "120/120 [==============================] - 0s 89us/sample - loss: 0.3618 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 299/500\n",
      "120/120 [==============================] - 0s 74us/sample - loss: 0.3718 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 300/500\n",
      "120/120 [==============================] - 0s 122us/sample - loss: 0.3682 - sparse_categorical_accuracy: 0.9333 - val_loss: 0.3514 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 301/500\n",
      "120/120 [==============================] - 0s 74us/sample - loss: 0.3612 - sparse_categorical_accuracy: 0.9000\n",
      "Epoch 302/500\n",
      "120/120 [==============================] - 0s 78us/sample - loss: 0.4212 - sparse_categorical_accuracy: 0.8667\n",
      "Epoch 303/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3701 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 304/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.3714 - sparse_categorical_accuracy: 0.9167\n",
      "Epoch 305/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3543 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 306/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.3445 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 307/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3452 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 308/500\n",
      "120/120 [==============================] - 0s 80us/sample - loss: 0.3441 - sparse_categorical_accuracy: 0.9833\n",
      "Epoch 309/500\n",
      "120/120 [==============================] - 0s 79us/sample - loss: 0.3593 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 310/500\n",
      "120/120 [==============================] - 0s 79us/sample - loss: 0.3656 - sparse_categorical_accuracy: 0.9167\n",
      "Epoch 311/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.3575 - sparse_categorical_accuracy: 0.9333\n",
      "Epoch 312/500\n",
      "120/120 [==============================] - 0s 90us/sample - loss: 0.3515 - sparse_categorical_accuracy: 0.9750\n",
      "Epoch 313/500\n",
      "120/120 [==============================] - 0s 89us/sample - loss: 0.3453 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 314/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3416 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 315/500\n",
      "120/120 [==============================] - 0s 93us/sample - loss: 0.3368 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 316/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.4223 - sparse_categorical_accuracy: 0.8917\n",
      "Epoch 317/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3694 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 318/500\n",
      "120/120 [==============================] - 0s 80us/sample - loss: 0.3409 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 319/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.3643 - sparse_categorical_accuracy: 0.9333\n",
      "Epoch 320/500\n",
      "120/120 [==============================] - 0s 123us/sample - loss: 0.3563 - sparse_categorical_accuracy: 0.9500 - val_loss: 0.6103 - val_sparse_categorical_accuracy: 0.6000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 321/500\n",
      "120/120 [==============================] - 0s 74us/sample - loss: 0.3852 - sparse_categorical_accuracy: 0.9167\n",
      "Epoch 322/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.3413 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 323/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3346 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 324/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3535 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 325/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.3591 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 326/500\n",
      "120/120 [==============================] - 0s 79us/sample - loss: 0.3533 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 327/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.3399 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 328/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.4023 - sparse_categorical_accuracy: 0.9000\n",
      "Epoch 329/500\n",
      "120/120 [==============================] - 0s 89us/sample - loss: 0.3758 - sparse_categorical_accuracy: 0.9167\n",
      "Epoch 330/500\n",
      "120/120 [==============================] - 0s 88us/sample - loss: 0.3763 - sparse_categorical_accuracy: 0.9333\n",
      "Epoch 331/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3627 - sparse_categorical_accuracy: 0.9000\n",
      "Epoch 332/500\n",
      "120/120 [==============================] - 0s 89us/sample - loss: 0.3408 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 333/500\n",
      "120/120 [==============================] - 0s 78us/sample - loss: 0.3571 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 334/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3856 - sparse_categorical_accuracy: 0.9167\n",
      "Epoch 335/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.4155 - sparse_categorical_accuracy: 0.8750\n",
      "Epoch 336/500\n",
      "120/120 [==============================] - 0s 79us/sample - loss: 0.5168 - sparse_categorical_accuracy: 0.7917\n",
      "Epoch 337/500\n",
      "120/120 [==============================] - 0s 82us/sample - loss: 0.3682 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 338/500\n",
      "120/120 [==============================] - 0s 100us/sample - loss: 0.3524 - sparse_categorical_accuracy: 0.9750\n",
      "Epoch 339/500\n",
      "120/120 [==============================] - 0s 93us/sample - loss: 0.3811 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 340/500\n",
      "120/120 [==============================] - 0s 125us/sample - loss: 0.3978 - sparse_categorical_accuracy: 0.8833 - val_loss: 0.4761 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 341/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3589 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 342/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3607 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 343/500\n",
      "120/120 [==============================] - 0s 78us/sample - loss: 0.3863 - sparse_categorical_accuracy: 0.9083\n",
      "Epoch 344/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3998 - sparse_categorical_accuracy: 0.9000\n",
      "Epoch 345/500\n",
      "120/120 [==============================] - 0s 78us/sample - loss: 0.3920 - sparse_categorical_accuracy: 0.9083\n",
      "Epoch 346/500\n",
      "120/120 [==============================] - 0s 90us/sample - loss: 0.4703 - sparse_categorical_accuracy: 0.8667\n",
      "Epoch 347/500\n",
      "120/120 [==============================] - 0s 90us/sample - loss: 0.4195 - sparse_categorical_accuracy: 0.8833\n",
      "Epoch 348/500\n",
      "120/120 [==============================] - 0s 87us/sample - loss: 0.3537 - sparse_categorical_accuracy: 0.9333\n",
      "Epoch 349/500\n",
      "120/120 [==============================] - 0s 87us/sample - loss: 0.3494 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 350/500\n",
      "120/120 [==============================] - 0s 88us/sample - loss: 0.3398 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 351/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3429 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 352/500\n",
      "120/120 [==============================] - 0s 90us/sample - loss: 0.3463 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 353/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.3714 - sparse_categorical_accuracy: 0.9167\n",
      "Epoch 354/500\n",
      "120/120 [==============================] - 0s 88us/sample - loss: 0.3735 - sparse_categorical_accuracy: 0.9000\n",
      "Epoch 355/500\n",
      "120/120 [==============================] - 0s 87us/sample - loss: 0.3949 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 356/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3822 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 357/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3570 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 358/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.4167 - sparse_categorical_accuracy: 0.8750\n",
      "Epoch 359/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.4038 - sparse_categorical_accuracy: 0.8583\n",
      "Epoch 360/500\n",
      "120/120 [==============================] - 0s 121us/sample - loss: 0.3636 - sparse_categorical_accuracy: 0.9417 - val_loss: 0.8138 - val_sparse_categorical_accuracy: 0.5333\n",
      "Epoch 361/500\n",
      "120/120 [==============================] - 0s 88us/sample - loss: 0.4337 - sparse_categorical_accuracy: 0.8667\n",
      "Epoch 362/500\n",
      "120/120 [==============================] - 0s 78us/sample - loss: 0.3853 - sparse_categorical_accuracy: 0.9167\n",
      "Epoch 363/500\n",
      "120/120 [==============================] - 0s 88us/sample - loss: 0.3598 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 364/500\n",
      "120/120 [==============================] - 0s 89us/sample - loss: 0.3588 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 365/500\n",
      "120/120 [==============================] - 0s 88us/sample - loss: 0.4300 - sparse_categorical_accuracy: 0.8667\n",
      "Epoch 366/500\n",
      "120/120 [==============================] - 0s 92us/sample - loss: 0.3912 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 367/500\n",
      "120/120 [==============================] - 0s 88us/sample - loss: 0.3395 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 368/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.4010 - sparse_categorical_accuracy: 0.8833\n",
      "Epoch 369/500\n",
      "120/120 [==============================] - 0s 90us/sample - loss: 0.3484 - sparse_categorical_accuracy: 0.9333\n",
      "Epoch 370/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3394 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 371/500\n",
      "120/120 [==============================] - 0s 88us/sample - loss: 0.3912 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 372/500\n",
      "120/120 [==============================] - 0s 88us/sample - loss: 0.3630 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 373/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3698 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 374/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.3446 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 375/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3443 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 376/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.4100 - sparse_categorical_accuracy: 0.9000\n",
      "Epoch 377/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3428 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 378/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3732 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 379/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.4839 - sparse_categorical_accuracy: 0.7917\n",
      "Epoch 380/500\n",
      "120/120 [==============================] - 0s 136us/sample - loss: 0.3773 - sparse_categorical_accuracy: 0.9083 - val_loss: 0.3185 - val_sparse_categorical_accuracy: 0.9667\n",
      "Epoch 381/500\n",
      "120/120 [==============================] - 0s 87us/sample - loss: 0.3405 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 382/500\n",
      "120/120 [==============================] - 0s 90us/sample - loss: 0.3427 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 383/500\n",
      "120/120 [==============================] - 0s 78us/sample - loss: 0.3512 - sparse_categorical_accuracy: 0.9333\n",
      "Epoch 384/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.3712 - sparse_categorical_accuracy: 0.9083\n",
      "Epoch 385/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3541 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 386/500\n",
      "120/120 [==============================] - 0s 79us/sample - loss: 0.4142 - sparse_categorical_accuracy: 0.8833\n",
      "Epoch 387/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3379 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 388/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3353 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 389/500\n",
      "120/120 [==============================] - 0s 89us/sample - loss: 0.3579 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 390/500\n",
      "120/120 [==============================] - 0s 78us/sample - loss: 0.3770 - sparse_categorical_accuracy: 0.9167\n",
      "Epoch 391/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3374 - sparse_categorical_accuracy: 0.9750\n",
      "Epoch 392/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.3372 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 393/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3448 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 394/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.3763 - sparse_categorical_accuracy: 0.9167\n",
      "Epoch 395/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.3595 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 396/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3794 - sparse_categorical_accuracy: 0.9167\n",
      "Epoch 397/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3636 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 398/500\n",
      "120/120 [==============================] - 0s 93us/sample - loss: 0.3539 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 399/500\n",
      "120/120 [==============================] - 0s 90us/sample - loss: 0.3476 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 400/500\n",
      "120/120 [==============================] - 0s 137us/sample - loss: 0.3774 - sparse_categorical_accuracy: 0.9167 - val_loss: 0.3298 - val_sparse_categorical_accuracy: 0.9667\n",
      "Epoch 401/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3674 - sparse_categorical_accuracy: 0.9000\n",
      "Epoch 402/500\n",
      "120/120 [==============================] - 0s 79us/sample - loss: 0.4001 - sparse_categorical_accuracy: 0.9167\n",
      "Epoch 403/500\n",
      "120/120 [==============================] - 0s 78us/sample - loss: 0.3456 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 404/500\n",
      "120/120 [==============================] - 0s 78us/sample - loss: 0.3913 - sparse_categorical_accuracy: 0.8667\n",
      "Epoch 405/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.3413 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 406/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3499 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 407/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.4235 - sparse_categorical_accuracy: 0.8833\n",
      "Epoch 408/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.3572 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 409/500\n",
      "120/120 [==============================] - 0s 79us/sample - loss: 0.3497 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 410/500\n",
      "120/120 [==============================] - 0s 78us/sample - loss: 0.3387 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 411/500\n",
      "120/120 [==============================] - 0s 82us/sample - loss: 0.3618 - sparse_categorical_accuracy: 0.9167\n",
      "Epoch 412/500\n",
      "120/120 [==============================] - 0s 78us/sample - loss: 0.3569 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 413/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3722 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 414/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.3410 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 415/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.3571 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 416/500\n",
      "120/120 [==============================] - 0s 78us/sample - loss: 0.3866 - sparse_categorical_accuracy: 0.9167\n",
      "Epoch 417/500\n",
      "120/120 [==============================] - 0s 91us/sample - loss: 0.3416 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 418/500\n",
      "120/120 [==============================] - 0s 92us/sample - loss: 0.3459 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 419/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.3700 - sparse_categorical_accuracy: 0.9167\n",
      "Epoch 420/500\n",
      "120/120 [==============================] - 0s 124us/sample - loss: 0.4336 - sparse_categorical_accuracy: 0.8917 - val_loss: 0.6770 - val_sparse_categorical_accuracy: 0.6000\n",
      "Epoch 421/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.4614 - sparse_categorical_accuracy: 0.8333\n",
      "Epoch 422/500\n",
      "120/120 [==============================] - 0s 78us/sample - loss: 0.3684 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 423/500\n",
      "120/120 [==============================] - 0s 79us/sample - loss: 0.3927 - sparse_categorical_accuracy: 0.8750\n",
      "Epoch 424/500\n",
      "120/120 [==============================] - 0s 87us/sample - loss: 0.3352 - sparse_categorical_accuracy: 0.9833\n",
      "Epoch 425/500\n",
      "120/120 [==============================] - 0s 101us/sample - loss: 0.3867 - sparse_categorical_accuracy: 0.8833\n",
      "Epoch 426/500\n",
      "120/120 [==============================] - 0s 79us/sample - loss: 0.3432 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 427/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.3410 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 428/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.3425 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 429/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3393 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 430/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3373 - sparse_categorical_accuracy: 0.9750\n",
      "Epoch 431/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3624 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 432/500\n",
      "120/120 [==============================] - 0s 91us/sample - loss: 0.3596 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 433/500\n",
      "120/120 [==============================] - 0s 78us/sample - loss: 0.3558 - sparse_categorical_accuracy: 0.9333\n",
      "Epoch 434/500\n",
      "120/120 [==============================] - 0s 91us/sample - loss: 0.3538 - sparse_categorical_accuracy: 0.9333\n",
      "Epoch 435/500\n",
      "120/120 [==============================] - 0s 90us/sample - loss: 0.3960 - sparse_categorical_accuracy: 0.8833\n",
      "Epoch 436/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3635 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 437/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.4245 - sparse_categorical_accuracy: 0.8583\n",
      "Epoch 438/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3392 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 439/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3492 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 440/500\n",
      "120/120 [==============================] - 0s 120us/sample - loss: 0.3371 - sparse_categorical_accuracy: 0.9583 - val_loss: 0.3438 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 441/500\n",
      "120/120 [==============================] - 0s 74us/sample - loss: 0.3701 - sparse_categorical_accuracy: 0.9167\n",
      "Epoch 442/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3417 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 443/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3538 - sparse_categorical_accuracy: 0.9333\n",
      "Epoch 444/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3401 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 445/500\n",
      "120/120 [==============================] - 0s 87us/sample - loss: 0.3775 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 446/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3463 - sparse_categorical_accuracy: 0.9333\n",
      "Epoch 447/500\n",
      "120/120 [==============================] - 0s 74us/sample - loss: 0.4087 - sparse_categorical_accuracy: 0.9000\n",
      "Epoch 448/500\n",
      "120/120 [==============================] - 0s 73us/sample - loss: 0.3768 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 449/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 89us/sample - loss: 0.3658 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 450/500\n",
      "120/120 [==============================] - 0s 88us/sample - loss: 0.3906 - sparse_categorical_accuracy: 0.9167\n",
      "Epoch 451/500\n",
      "120/120 [==============================] - 0s 90us/sample - loss: 0.3470 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 452/500\n",
      "120/120 [==============================] - 0s 88us/sample - loss: 0.3467 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 453/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3642 - sparse_categorical_accuracy: 0.9083\n",
      "Epoch 454/500\n",
      "120/120 [==============================] - 0s 74us/sample - loss: 0.3548 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 455/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3693 - sparse_categorical_accuracy: 0.9167\n",
      "Epoch 456/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3876 - sparse_categorical_accuracy: 0.9083\n",
      "Epoch 457/500\n",
      "120/120 [==============================] - 0s 74us/sample - loss: 0.3648 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 458/500\n",
      "120/120 [==============================] - 0s 88us/sample - loss: 0.4028 - sparse_categorical_accuracy: 0.8833\n",
      "Epoch 459/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3596 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 460/500\n",
      "120/120 [==============================] - 0s 123us/sample - loss: 0.4119 - sparse_categorical_accuracy: 0.8833 - val_loss: 0.5965 - val_sparse_categorical_accuracy: 0.6000\n",
      "Epoch 461/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3860 - sparse_categorical_accuracy: 0.8833\n",
      "Epoch 462/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.4139 - sparse_categorical_accuracy: 0.8750\n",
      "Epoch 463/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3460 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 464/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3623 - sparse_categorical_accuracy: 0.9333\n",
      "Epoch 465/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3343 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 466/500\n",
      "120/120 [==============================] - 0s 89us/sample - loss: 0.3910 - sparse_categorical_accuracy: 0.9083\n",
      "Epoch 467/500\n",
      "120/120 [==============================] - 0s 88us/sample - loss: 0.4764 - sparse_categorical_accuracy: 0.8250\n",
      "Epoch 468/500\n",
      "120/120 [==============================] - 0s 88us/sample - loss: 0.3636 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 469/500\n",
      "120/120 [==============================] - 0s 88us/sample - loss: 0.3425 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 470/500\n",
      "120/120 [==============================] - 0s 74us/sample - loss: 0.3753 - sparse_categorical_accuracy: 0.9333\n",
      "Epoch 471/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.4553 - sparse_categorical_accuracy: 0.8167\n",
      "Epoch 472/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.3502 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 473/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3874 - sparse_categorical_accuracy: 0.9000\n",
      "Epoch 474/500\n",
      "120/120 [==============================] - 0s 79us/sample - loss: 0.4106 - sparse_categorical_accuracy: 0.8667\n",
      "Epoch 475/500\n",
      "120/120 [==============================] - 0s 89us/sample - loss: 0.3458 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 476/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.3452 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 477/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3426 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 478/500\n",
      "120/120 [==============================] - 0s 89us/sample - loss: 0.3489 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 479/500\n",
      "120/120 [==============================] - 0s 88us/sample - loss: 0.3354 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 480/500\n",
      "120/120 [==============================] - 0s 136us/sample - loss: 0.3357 - sparse_categorical_accuracy: 0.9833 - val_loss: 0.3341 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 481/500\n",
      "120/120 [==============================] - 0s 87us/sample - loss: 0.3943 - sparse_categorical_accuracy: 0.9000\n",
      "Epoch 482/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3412 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 483/500\n",
      "120/120 [==============================] - 0s 91us/sample - loss: 0.3632 - sparse_categorical_accuracy: 0.9333\n",
      "Epoch 484/500\n",
      "120/120 [==============================] - 0s 88us/sample - loss: 0.3523 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 485/500\n",
      "120/120 [==============================] - 0s 90us/sample - loss: 0.3508 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 486/500\n",
      "120/120 [==============================] - 0s 88us/sample - loss: 0.3649 - sparse_categorical_accuracy: 0.9167\n",
      "Epoch 487/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3386 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 488/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3598 - sparse_categorical_accuracy: 0.9333\n",
      "Epoch 489/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.4113 - sparse_categorical_accuracy: 0.8667\n",
      "Epoch 490/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.4515 - sparse_categorical_accuracy: 0.8500\n",
      "Epoch 491/500\n",
      "120/120 [==============================] - 0s 89us/sample - loss: 0.4595 - sparse_categorical_accuracy: 0.8500\n",
      "Epoch 492/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3822 - sparse_categorical_accuracy: 0.9083\n",
      "Epoch 493/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3504 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 494/500\n",
      "120/120 [==============================] - 0s 88us/sample - loss: 0.3589 - sparse_categorical_accuracy: 0.9083\n",
      "Epoch 495/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.5808 - sparse_categorical_accuracy: 0.7333\n",
      "Epoch 496/500\n",
      "120/120 [==============================] - 0s 90us/sample - loss: 0.3468 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 497/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.3640 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 498/500\n",
      "120/120 [==============================] - 0s 88us/sample - loss: 0.3336 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 499/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3486 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 500/500\n",
      "120/120 [==============================] - 0s 133us/sample - loss: 0.3333 - sparse_categorical_accuracy: 0.9667 - val_loss: 0.4002 - val_sparse_categorical_accuracy: 1.0000\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              multiple                  15        \n",
      "=================================================================\n",
      "Total params: 15\n",
      "Trainable params: 15\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#import\n",
    "import tensorflow as tf\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "\n",
    "#train test\n",
    "x_train = datasets.load_iris().data\n",
    "y_train = datasets.load_iris().target\n",
    "\n",
    "np.random.seed(116)\n",
    "np.random.shuffle(x_train)\n",
    "np.random.seed(116)\n",
    "np.random.shuffle(y_train)\n",
    "tf.random.set_seed(116)\n",
    "\n",
    "#models Sequential\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(3, activation='softmax', kernel_regularizer=tf.keras.regularizers.l2())\n",
    "])\n",
    "\n",
    "#model compile\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(lr=0.1),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "              metrics=['sparse_categorical_accuracy'])\n",
    "\n",
    "#model fit\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=500, validation_split=0.2, validation_freq=20)\n",
    "\n",
    "#model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用自定义类完成鸢尾花分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 120 samples, validate on 30 samples\n",
      "Epoch 1/500\n",
      "120/120 [==============================] - 0s 3ms/sample - loss: 2.1962 - sparse_categorical_accuracy: 0.3500\n",
      "Epoch 2/500\n",
      "120/120 [==============================] - 0s 81us/sample - loss: 0.8823 - sparse_categorical_accuracy: 0.6417\n",
      "Epoch 3/500\n",
      "120/120 [==============================] - 0s 78us/sample - loss: 1.0519 - sparse_categorical_accuracy: 0.6500\n",
      "Epoch 4/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.8097 - sparse_categorical_accuracy: 0.6333\n",
      "Epoch 5/500\n",
      "120/120 [==============================] - 0s 79us/sample - loss: 1.0982 - sparse_categorical_accuracy: 0.6250\n",
      "Epoch 6/500\n",
      "120/120 [==============================] - 0s 86us/sample - loss: 0.6229 - sparse_categorical_accuracy: 0.7167\n",
      "Epoch 7/500\n",
      "120/120 [==============================] - 0s 78us/sample - loss: 0.9548 - sparse_categorical_accuracy: 0.6333\n",
      "Epoch 8/500\n",
      "120/120 [==============================] - 0s 78us/sample - loss: 0.5668 - sparse_categorical_accuracy: 0.7417\n",
      "Epoch 9/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.6024 - sparse_categorical_accuracy: 0.7167\n",
      "Epoch 10/500\n",
      "120/120 [==============================] - 0s 83us/sample - loss: 0.5653 - sparse_categorical_accuracy: 0.7750\n",
      "Epoch 11/500\n",
      "120/120 [==============================] - 0s 79us/sample - loss: 0.5925 - sparse_categorical_accuracy: 0.7250\n",
      "Epoch 12/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.6456 - sparse_categorical_accuracy: 0.6667\n",
      "Epoch 13/500\n",
      "120/120 [==============================] - 0s 74us/sample - loss: 0.5960 - sparse_categorical_accuracy: 0.7250\n",
      "Epoch 14/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.6188 - sparse_categorical_accuracy: 0.6750\n",
      "Epoch 15/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.5207 - sparse_categorical_accuracy: 0.8417\n",
      "Epoch 16/500\n",
      "120/120 [==============================] - 0s 81us/sample - loss: 0.5884 - sparse_categorical_accuracy: 0.6833\n",
      "Epoch 17/500\n",
      "120/120 [==============================] - 0s 74us/sample - loss: 0.4988 - sparse_categorical_accuracy: 0.8083\n",
      "Epoch 18/500\n",
      "120/120 [==============================] - 0s 79us/sample - loss: 0.5115 - sparse_categorical_accuracy: 0.7583\n",
      "Epoch 19/500\n",
      "120/120 [==============================] - 0s 98us/sample - loss: 0.6111 - sparse_categorical_accuracy: 0.6917\n",
      "Epoch 20/500\n",
      "120/120 [==============================] - 0s 1ms/sample - loss: 0.7344 - sparse_categorical_accuracy: 0.6833 - val_loss: 0.6025 - val_sparse_categorical_accuracy: 0.5333\n",
      "Epoch 21/500\n",
      "120/120 [==============================] - 0s 82us/sample - loss: 0.4730 - sparse_categorical_accuracy: 0.8000\n",
      "Epoch 22/500\n",
      "120/120 [==============================] - 0s 78us/sample - loss: 0.6484 - sparse_categorical_accuracy: 0.6750\n",
      "Epoch 23/500\n",
      "120/120 [==============================] - 0s 91us/sample - loss: 0.5528 - sparse_categorical_accuracy: 0.7917\n",
      "Epoch 24/500\n",
      "120/120 [==============================] - 0s 82us/sample - loss: 0.7129 - sparse_categorical_accuracy: 0.6500\n",
      "Epoch 25/500\n",
      "120/120 [==============================] - 0s 91us/sample - loss: 0.5139 - sparse_categorical_accuracy: 0.7667\n",
      "Epoch 26/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.4982 - sparse_categorical_accuracy: 0.8000\n",
      "Epoch 27/500\n",
      "120/120 [==============================] - 0s 74us/sample - loss: 1.1382 - sparse_categorical_accuracy: 0.5917\n",
      "Epoch 28/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.5493 - sparse_categorical_accuracy: 0.7000\n",
      "Epoch 29/500\n",
      "120/120 [==============================] - 0s 78us/sample - loss: 0.6210 - sparse_categorical_accuracy: 0.7000\n",
      "Epoch 30/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.5422 - sparse_categorical_accuracy: 0.7000\n",
      "Epoch 31/500\n",
      "120/120 [==============================] - 0s 104us/sample - loss: 0.4212 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 32/500\n",
      "120/120 [==============================] - 0s 66us/sample - loss: 0.5288 - sparse_categorical_accuracy: 0.7000\n",
      "Epoch 33/500\n",
      "120/120 [==============================] - 0s 168us/sample - loss: 0.6907 - sparse_categorical_accuracy: 0.6583\n",
      "Epoch 34/500\n",
      "120/120 [==============================] - 0s 66us/sample - loss: 0.4772 - sparse_categorical_accuracy: 0.8083\n",
      "Epoch 35/500\n",
      "120/120 [==============================] - 0s 216us/sample - loss: 0.4436 - sparse_categorical_accuracy: 0.8750\n",
      "Epoch 36/500\n",
      "120/120 [==============================] - 0s 69us/sample - loss: 0.5897 - sparse_categorical_accuracy: 0.7417\n",
      "Epoch 37/500\n",
      "120/120 [==============================] - 0s 67us/sample - loss: 0.4435 - sparse_categorical_accuracy: 0.8250\n",
      "Epoch 38/500\n",
      "120/120 [==============================] - 0s 64us/sample - loss: 0.4061 - sparse_categorical_accuracy: 0.8833\n",
      "Epoch 39/500\n",
      "120/120 [==============================] - 0s 68us/sample - loss: 0.4785 - sparse_categorical_accuracy: 0.7583\n",
      "Epoch 40/500\n",
      "120/120 [==============================] - 0s 140us/sample - loss: 0.4328 - sparse_categorical_accuracy: 0.8417 - val_loss: 0.5688 - val_sparse_categorical_accuracy: 0.6000\n",
      "Epoch 41/500\n",
      "120/120 [==============================] - 0s 73us/sample - loss: 0.4749 - sparse_categorical_accuracy: 0.7917\n",
      "Epoch 42/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.5713 - sparse_categorical_accuracy: 0.7167\n",
      "Epoch 43/500\n",
      "120/120 [==============================] - 0s 83us/sample - loss: 0.5949 - sparse_categorical_accuracy: 0.7000\n",
      "Epoch 44/500\n",
      "120/120 [==============================] - 0s 91us/sample - loss: 0.3894 - sparse_categorical_accuracy: 0.8750\n",
      "Epoch 45/500\n",
      "120/120 [==============================] - 0s 90us/sample - loss: 0.6079 - sparse_categorical_accuracy: 0.7333\n",
      "Epoch 46/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.6114 - sparse_categorical_accuracy: 0.6917\n",
      "Epoch 47/500\n",
      "120/120 [==============================] - 0s 87us/sample - loss: 0.4088 - sparse_categorical_accuracy: 0.8833\n",
      "Epoch 48/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.4352 - sparse_categorical_accuracy: 0.8750\n",
      "Epoch 49/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.5262 - sparse_categorical_accuracy: 0.7417\n",
      "Epoch 50/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.4200 - sparse_categorical_accuracy: 0.8917\n",
      "Epoch 51/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.6255 - sparse_categorical_accuracy: 0.6583\n",
      "Epoch 52/500\n",
      "120/120 [==============================] - 0s 79us/sample - loss: 0.8264 - sparse_categorical_accuracy: 0.6833\n",
      "Epoch 53/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.4714 - sparse_categorical_accuracy: 0.8250\n",
      "Epoch 54/500\n",
      "120/120 [==============================] - 0s 90us/sample - loss: 0.5078 - sparse_categorical_accuracy: 0.7500\n",
      "Epoch 55/500\n",
      "120/120 [==============================] - 0s 74us/sample - loss: 0.4240 - sparse_categorical_accuracy: 0.8750\n",
      "Epoch 56/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.4657 - sparse_categorical_accuracy: 0.8583\n",
      "Epoch 57/500\n",
      "120/120 [==============================] - 0s 74us/sample - loss: 0.3779 - sparse_categorical_accuracy: 0.9167\n",
      "Epoch 58/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3928 - sparse_categorical_accuracy: 0.9083\n",
      "Epoch 59/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3994 - sparse_categorical_accuracy: 0.8917\n",
      "Epoch 60/500\n",
      "120/120 [==============================] - 0s 124us/sample - loss: 0.4248 - sparse_categorical_accuracy: 0.8667 - val_loss: 0.9493 - val_sparse_categorical_accuracy: 0.5333\n",
      "Epoch 61/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.7443 - sparse_categorical_accuracy: 0.6833\n",
      "Epoch 62/500\n",
      "120/120 [==============================] - 0s 87us/sample - loss: 0.4022 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 63/500\n",
      "120/120 [==============================] - 0s 73us/sample - loss: 0.4407 - sparse_categorical_accuracy: 0.8500\n",
      "Epoch 64/500\n",
      "120/120 [==============================] - 0s 87us/sample - loss: 0.4254 - sparse_categorical_accuracy: 0.8583\n",
      "Epoch 65/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 74us/sample - loss: 0.4211 - sparse_categorical_accuracy: 0.8667\n",
      "Epoch 66/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3679 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 67/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.4784 - sparse_categorical_accuracy: 0.7833\n",
      "Epoch 68/500\n",
      "120/120 [==============================] - 0s 91us/sample - loss: 0.6290 - sparse_categorical_accuracy: 0.6750\n",
      "Epoch 69/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3813 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 70/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3755 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 71/500\n",
      "120/120 [==============================] - 0s 87us/sample - loss: 0.3886 - sparse_categorical_accuracy: 0.9167\n",
      "Epoch 72/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.4446 - sparse_categorical_accuracy: 0.8167\n",
      "Epoch 73/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3933 - sparse_categorical_accuracy: 0.9333\n",
      "Epoch 74/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.4546 - sparse_categorical_accuracy: 0.8583\n",
      "Epoch 75/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3730 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 76/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3745 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 77/500\n",
      "120/120 [==============================] - 0s 78us/sample - loss: 0.3885 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 78/500\n",
      "120/120 [==============================] - 0s 88us/sample - loss: 0.6013 - sparse_categorical_accuracy: 0.7083\n",
      "Epoch 79/500\n",
      "120/120 [==============================] - 0s 74us/sample - loss: 0.3682 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 80/500\n",
      "120/120 [==============================] - 0s 139us/sample - loss: 0.3648 - sparse_categorical_accuracy: 0.9333 - val_loss: 0.3486 - val_sparse_categorical_accuracy: 0.9667\n",
      "Epoch 81/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.5549 - sparse_categorical_accuracy: 0.7333\n",
      "Epoch 82/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.4028 - sparse_categorical_accuracy: 0.9167\n",
      "Epoch 83/500\n",
      "120/120 [==============================] - 0s 80us/sample - loss: 0.3798 - sparse_categorical_accuracy: 0.9083\n",
      "Epoch 84/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3935 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 85/500\n",
      "120/120 [==============================] - 0s 79us/sample - loss: 0.4358 - sparse_categorical_accuracy: 0.8500\n",
      "Epoch 86/500\n",
      "120/120 [==============================] - 0s 82us/sample - loss: 0.4759 - sparse_categorical_accuracy: 0.8250\n",
      "Epoch 87/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.4273 - sparse_categorical_accuracy: 0.8917\n",
      "Epoch 88/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.5257 - sparse_categorical_accuracy: 0.7500\n",
      "Epoch 89/500\n",
      "120/120 [==============================] - 0s 79us/sample - loss: 0.5146 - sparse_categorical_accuracy: 0.8000\n",
      "Epoch 90/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.4159 - sparse_categorical_accuracy: 0.9083\n",
      "Epoch 91/500\n",
      "120/120 [==============================] - 0s 81us/sample - loss: 0.4119 - sparse_categorical_accuracy: 0.8667\n",
      "Epoch 92/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.3800 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 93/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.4675 - sparse_categorical_accuracy: 0.7917\n",
      "Epoch 94/500\n",
      "120/120 [==============================] - 0s 101us/sample - loss: 0.4614 - sparse_categorical_accuracy: 0.8167\n",
      "Epoch 95/500\n",
      "120/120 [==============================] - 0s 93us/sample - loss: 0.3742 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 96/500\n",
      "120/120 [==============================] - 0s 89us/sample - loss: 0.3666 - sparse_categorical_accuracy: 0.9333\n",
      "Epoch 97/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3699 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 98/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.3871 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 99/500\n",
      "120/120 [==============================] - 0s 74us/sample - loss: 0.3991 - sparse_categorical_accuracy: 0.9083\n",
      "Epoch 100/500\n",
      "120/120 [==============================] - 0s 122us/sample - loss: 0.3707 - sparse_categorical_accuracy: 0.9333 - val_loss: 0.3488 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 101/500\n",
      "120/120 [==============================] - 0s 91us/sample - loss: 0.3660 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 102/500\n",
      "120/120 [==============================] - 0s 79us/sample - loss: 0.3743 - sparse_categorical_accuracy: 0.9167\n",
      "Epoch 103/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.4286 - sparse_categorical_accuracy: 0.8500\n",
      "Epoch 104/500\n",
      "120/120 [==============================] - 0s 73us/sample - loss: 0.4726 - sparse_categorical_accuracy: 0.8083\n",
      "Epoch 105/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3971 - sparse_categorical_accuracy: 0.9083\n",
      "Epoch 106/500\n",
      "120/120 [==============================] - 0s 86us/sample - loss: 0.3626 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 107/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3853 - sparse_categorical_accuracy: 0.8917\n",
      "Epoch 108/500\n",
      "120/120 [==============================] - 0s 91us/sample - loss: 0.4205 - sparse_categorical_accuracy: 0.8750\n",
      "Epoch 109/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.4696 - sparse_categorical_accuracy: 0.8333\n",
      "Epoch 110/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.4009 - sparse_categorical_accuracy: 0.8917\n",
      "Epoch 111/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.4237 - sparse_categorical_accuracy: 0.8667\n",
      "Epoch 112/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3955 - sparse_categorical_accuracy: 0.9167\n",
      "Epoch 113/500\n",
      "120/120 [==============================] - 0s 87us/sample - loss: 0.3932 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 114/500\n",
      "120/120 [==============================] - 0s 74us/sample - loss: 0.3771 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 115/500\n",
      "120/120 [==============================] - 0s 73us/sample - loss: 0.4146 - sparse_categorical_accuracy: 0.8917\n",
      "Epoch 116/500\n",
      "120/120 [==============================] - 0s 73us/sample - loss: 0.3639 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 117/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3559 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 118/500\n",
      "120/120 [==============================] - 0s 90us/sample - loss: 0.4085 - sparse_categorical_accuracy: 0.8833\n",
      "Epoch 119/500\n",
      "120/120 [==============================] - 0s 73us/sample - loss: 0.4038 - sparse_categorical_accuracy: 0.8833\n",
      "Epoch 120/500\n",
      "120/120 [==============================] - 0s 120us/sample - loss: 0.3898 - sparse_categorical_accuracy: 0.9083 - val_loss: 0.3566 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 121/500\n",
      "120/120 [==============================] - 0s 86us/sample - loss: 0.3968 - sparse_categorical_accuracy: 0.9083\n",
      "Epoch 122/500\n",
      "120/120 [==============================] - 0s 74us/sample - loss: 0.5416 - sparse_categorical_accuracy: 0.8083\n",
      "Epoch 123/500\n",
      "120/120 [==============================] - 0s 88us/sample - loss: 0.4635 - sparse_categorical_accuracy: 0.8500\n",
      "Epoch 124/500\n",
      "120/120 [==============================] - 0s 74us/sample - loss: 0.3970 - sparse_categorical_accuracy: 0.9000\n",
      "Epoch 125/500\n",
      "120/120 [==============================] - 0s 90us/sample - loss: 0.4089 - sparse_categorical_accuracy: 0.8667\n",
      "Epoch 126/500\n",
      "120/120 [==============================] - 0s 91us/sample - loss: 0.3783 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 127/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.4521 - sparse_categorical_accuracy: 0.8667\n",
      "Epoch 128/500\n",
      "120/120 [==============================] - 0s 72us/sample - loss: 0.3612 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 129/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3534 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 130/500\n",
      "120/120 [==============================] - 0s 86us/sample - loss: 0.3624 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 131/500\n",
      "120/120 [==============================] - 0s 74us/sample - loss: 0.3767 - sparse_categorical_accuracy: 0.9083\n",
      "Epoch 132/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.4185 - sparse_categorical_accuracy: 0.8750\n",
      "Epoch 133/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3814 - sparse_categorical_accuracy: 0.9167\n",
      "Epoch 134/500\n",
      "120/120 [==============================] - 0s 81us/sample - loss: 0.4230 - sparse_categorical_accuracy: 0.8917\n",
      "Epoch 135/500\n",
      "120/120 [==============================] - 0s 78us/sample - loss: 0.3636 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 136/500\n",
      "120/120 [==============================] - 0s 74us/sample - loss: 0.3685 - sparse_categorical_accuracy: 0.9333\n",
      "Epoch 137/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.3573 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 138/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3876 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 139/500\n",
      "120/120 [==============================] - 0s 88us/sample - loss: 0.3797 - sparse_categorical_accuracy: 0.9167\n",
      "Epoch 140/500\n",
      "120/120 [==============================] - 0s 119us/sample - loss: 0.3610 - sparse_categorical_accuracy: 0.9167 - val_loss: 0.3956 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 141/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3889 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 142/500\n",
      "120/120 [==============================] - 0s 94us/sample - loss: 0.3711 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 143/500\n",
      "120/120 [==============================] - 0s 86us/sample - loss: 0.3936 - sparse_categorical_accuracy: 0.9000\n",
      "Epoch 144/500\n",
      "120/120 [==============================] - 0s 74us/sample - loss: 0.4222 - sparse_categorical_accuracy: 0.8833\n",
      "Epoch 145/500\n",
      "120/120 [==============================] - 0s 78us/sample - loss: 0.3865 - sparse_categorical_accuracy: 0.9083\n",
      "Epoch 146/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3884 - sparse_categorical_accuracy: 0.9000\n",
      "Epoch 147/500\n",
      "120/120 [==============================] - 0s 88us/sample - loss: 0.3573 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 148/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3567 - sparse_categorical_accuracy: 0.9333\n",
      "Epoch 149/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.3535 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 150/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3700 - sparse_categorical_accuracy: 0.9083\n",
      "Epoch 151/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3543 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 152/500\n",
      "120/120 [==============================] - 0s 78us/sample - loss: 0.3525 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 153/500\n",
      "120/120 [==============================] - 0s 81us/sample - loss: 0.3923 - sparse_categorical_accuracy: 0.9083\n",
      "Epoch 154/500\n",
      "120/120 [==============================] - 0s 79us/sample - loss: 0.3644 - sparse_categorical_accuracy: 0.9333\n",
      "Epoch 155/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3598 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 156/500\n",
      "120/120 [==============================] - 0s 73us/sample - loss: 0.3713 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 157/500\n",
      "120/120 [==============================] - 0s 74us/sample - loss: 0.3599 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 158/500\n",
      "120/120 [==============================] - 0s 88us/sample - loss: 0.3831 - sparse_categorical_accuracy: 0.9333\n",
      "Epoch 159/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.4198 - sparse_categorical_accuracy: 0.8917\n",
      "Epoch 160/500\n",
      "120/120 [==============================] - 0s 140us/sample - loss: 0.3753 - sparse_categorical_accuracy: 0.9083 - val_loss: 0.3352 - val_sparse_categorical_accuracy: 0.9667\n",
      "Epoch 161/500\n",
      "120/120 [==============================] - 0s 74us/sample - loss: 0.3496 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 162/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3552 - sparse_categorical_accuracy: 0.9333\n",
      "Epoch 163/500\n",
      "120/120 [==============================] - 0s 74us/sample - loss: 0.3585 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 164/500\n",
      "120/120 [==============================] - 0s 86us/sample - loss: 0.3939 - sparse_categorical_accuracy: 0.8917\n",
      "Epoch 165/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3553 - sparse_categorical_accuracy: 0.9750\n",
      "Epoch 166/500\n",
      "120/120 [==============================] - 0s 74us/sample - loss: 0.4774 - sparse_categorical_accuracy: 0.8167\n",
      "Epoch 167/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3585 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 168/500\n",
      "120/120 [==============================] - 0s 78us/sample - loss: 0.3671 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 169/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3551 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 170/500\n",
      "120/120 [==============================] - 0s 74us/sample - loss: 0.3679 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 171/500\n",
      "120/120 [==============================] - 0s 88us/sample - loss: 0.3501 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 172/500\n",
      "120/120 [==============================] - 0s 74us/sample - loss: 0.3583 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 173/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.3850 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 174/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3711 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 175/500\n",
      "120/120 [==============================] - 0s 89us/sample - loss: 0.5982 - sparse_categorical_accuracy: 0.7583\n",
      "Epoch 176/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.4060 - sparse_categorical_accuracy: 0.8750\n",
      "Epoch 177/500\n",
      "120/120 [==============================] - 0s 89us/sample - loss: 0.3808 - sparse_categorical_accuracy: 0.9083\n",
      "Epoch 178/500\n",
      "120/120 [==============================] - 0s 89us/sample - loss: 0.3528 - sparse_categorical_accuracy: 0.9333\n",
      "Epoch 179/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3529 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 180/500\n",
      "120/120 [==============================] - 0s 120us/sample - loss: 0.5415 - sparse_categorical_accuracy: 0.8000 - val_loss: 0.3710 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 181/500\n",
      "120/120 [==============================] - 0s 72us/sample - loss: 0.3991 - sparse_categorical_accuracy: 0.8833\n",
      "Epoch 182/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.3712 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 183/500\n",
      "120/120 [==============================] - 0s 78us/sample - loss: 0.4376 - sparse_categorical_accuracy: 0.8833\n",
      "Epoch 184/500\n",
      "120/120 [==============================] - 0s 79us/sample - loss: 0.3740 - sparse_categorical_accuracy: 0.9333\n",
      "Epoch 185/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3531 - sparse_categorical_accuracy: 0.9750\n",
      "Epoch 186/500\n",
      "120/120 [==============================] - 0s 78us/sample - loss: 0.4468 - sparse_categorical_accuracy: 0.8667\n",
      "Epoch 187/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.3520 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 188/500\n",
      "120/120 [==============================] - 0s 78us/sample - loss: 0.3960 - sparse_categorical_accuracy: 0.9083\n",
      "Epoch 189/500\n",
      "120/120 [==============================] - 0s 79us/sample - loss: 0.3752 - sparse_categorical_accuracy: 0.9083\n",
      "Epoch 190/500\n",
      "120/120 [==============================] - 0s 81us/sample - loss: 0.3450 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 191/500\n",
      "120/120 [==============================] - 0s 78us/sample - loss: 0.3676 - sparse_categorical_accuracy: 0.9083\n",
      "Epoch 192/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3965 - sparse_categorical_accuracy: 0.8917\n",
      "Epoch 193/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 77us/sample - loss: 0.4040 - sparse_categorical_accuracy: 0.8917\n",
      "Epoch 194/500\n",
      "120/120 [==============================] - 0s 89us/sample - loss: 0.3535 - sparse_categorical_accuracy: 0.9333\n",
      "Epoch 195/500\n",
      "120/120 [==============================] - 0s 87us/sample - loss: 0.3798 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 196/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3599 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 197/500\n",
      "120/120 [==============================] - 0s 74us/sample - loss: 0.3632 - sparse_categorical_accuracy: 0.9333\n",
      "Epoch 198/500\n",
      "120/120 [==============================] - 0s 107us/sample - loss: 0.3789 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 199/500\n",
      "120/120 [==============================] - 0s 115us/sample - loss: 0.3995 - sparse_categorical_accuracy: 0.8750\n",
      "Epoch 200/500\n",
      "120/120 [==============================] - 0s 142us/sample - loss: 0.3566 - sparse_categorical_accuracy: 0.9333 - val_loss: 0.3405 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 201/500\n",
      "120/120 [==============================] - 0s 72us/sample - loss: 0.3695 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 202/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3705 - sparse_categorical_accuracy: 0.9083\n",
      "Epoch 203/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.3659 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 204/500\n",
      "120/120 [==============================] - 0s 73us/sample - loss: 0.3444 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 205/500\n",
      "120/120 [==============================] - 0s 85us/sample - loss: 0.3482 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 206/500\n",
      "120/120 [==============================] - 0s 86us/sample - loss: 0.3541 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 207/500\n",
      "120/120 [==============================] - 0s 74us/sample - loss: 0.3461 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 208/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.4319 - sparse_categorical_accuracy: 0.8667\n",
      "Epoch 209/500\n",
      "120/120 [==============================] - 0s 79us/sample - loss: 0.3794 - sparse_categorical_accuracy: 0.9083\n",
      "Epoch 210/500\n",
      "120/120 [==============================] - 0s 89us/sample - loss: 0.4089 - sparse_categorical_accuracy: 0.8667\n",
      "Epoch 211/500\n",
      "120/120 [==============================] - 0s 87us/sample - loss: 0.4438 - sparse_categorical_accuracy: 0.8417\n",
      "Epoch 212/500\n",
      "120/120 [==============================] - 0s 73us/sample - loss: 0.3817 - sparse_categorical_accuracy: 0.9167\n",
      "Epoch 213/500\n",
      "120/120 [==============================] - 0s 73us/sample - loss: 0.3611 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 214/500\n",
      "120/120 [==============================] - 0s 86us/sample - loss: 0.3471 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 215/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3721 - sparse_categorical_accuracy: 0.9167\n",
      "Epoch 216/500\n",
      "120/120 [==============================] - 0s 73us/sample - loss: 0.3789 - sparse_categorical_accuracy: 0.9167\n",
      "Epoch 217/500\n",
      "120/120 [==============================] - 0s 90us/sample - loss: 0.3548 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 218/500\n",
      "120/120 [==============================] - 0s 92us/sample - loss: 0.3957 - sparse_categorical_accuracy: 0.8917\n",
      "Epoch 219/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3922 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 220/500\n",
      "120/120 [==============================] - 0s 119us/sample - loss: 0.4800 - sparse_categorical_accuracy: 0.8250 - val_loss: 0.3715 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 221/500\n",
      "120/120 [==============================] - 0s 71us/sample - loss: 0.3449 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 222/500\n",
      "120/120 [==============================] - 0s 73us/sample - loss: 0.3515 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 223/500\n",
      "120/120 [==============================] - 0s 88us/sample - loss: 0.3992 - sparse_categorical_accuracy: 0.9083\n",
      "Epoch 224/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.4384 - sparse_categorical_accuracy: 0.8750\n",
      "Epoch 225/500\n",
      "120/120 [==============================] - 0s 79us/sample - loss: 0.4206 - sparse_categorical_accuracy: 0.9083\n",
      "Epoch 226/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.3515 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 227/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3643 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 228/500\n",
      "120/120 [==============================] - 0s 88us/sample - loss: 0.3677 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 229/500\n",
      "120/120 [==============================] - 0s 73us/sample - loss: 0.3581 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 230/500\n",
      "120/120 [==============================] - 0s 74us/sample - loss: 0.3595 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 231/500\n",
      "120/120 [==============================] - 0s 87us/sample - loss: 0.4021 - sparse_categorical_accuracy: 0.8917\n",
      "Epoch 232/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3488 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 233/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.4141 - sparse_categorical_accuracy: 0.8667\n",
      "Epoch 234/500\n",
      "120/120 [==============================] - 0s 90us/sample - loss: 0.3510 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 235/500\n",
      "120/120 [==============================] - 0s 90us/sample - loss: 0.3453 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 236/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3707 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 237/500\n",
      "120/120 [==============================] - 0s 74us/sample - loss: 0.3780 - sparse_categorical_accuracy: 0.9167\n",
      "Epoch 238/500\n",
      "120/120 [==============================] - 0s 87us/sample - loss: 0.3629 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 239/500\n",
      "120/120 [==============================] - 0s 93us/sample - loss: 0.3612 - sparse_categorical_accuracy: 0.9333\n",
      "Epoch 240/500\n",
      "120/120 [==============================] - 0s 138us/sample - loss: 0.3823 - sparse_categorical_accuracy: 0.9333 - val_loss: 0.3478 - val_sparse_categorical_accuracy: 0.9000\n",
      "Epoch 241/500\n",
      "120/120 [==============================] - 0s 94us/sample - loss: 0.3899 - sparse_categorical_accuracy: 0.8917\n",
      "Epoch 242/500\n",
      "120/120 [==============================] - 0s 92us/sample - loss: 0.4695 - sparse_categorical_accuracy: 0.8167\n",
      "Epoch 243/500\n",
      "120/120 [==============================] - 0s 78us/sample - loss: 0.3549 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 244/500\n",
      "120/120 [==============================] - 0s 79us/sample - loss: 0.4236 - sparse_categorical_accuracy: 0.8667\n",
      "Epoch 245/500\n",
      "120/120 [==============================] - 0s 90us/sample - loss: 0.4944 - sparse_categorical_accuracy: 0.8583\n",
      "Epoch 246/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3543 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 247/500\n",
      "120/120 [==============================] - 0s 88us/sample - loss: 0.3524 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 248/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3801 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 249/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3485 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 250/500\n",
      "120/120 [==============================] - 0s 87us/sample - loss: 0.3434 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 251/500\n",
      "120/120 [==============================] - 0s 87us/sample - loss: 0.3485 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 252/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3454 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 253/500\n",
      "120/120 [==============================] - 0s 74us/sample - loss: 0.3492 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 254/500\n",
      "120/120 [==============================] - 0s 85us/sample - loss: 0.3815 - sparse_categorical_accuracy: 0.8917\n",
      "Epoch 255/500\n",
      "120/120 [==============================] - 0s 87us/sample - loss: 0.4077 - sparse_categorical_accuracy: 0.8833\n",
      "Epoch 256/500\n",
      "120/120 [==============================] - 0s 86us/sample - loss: 0.3796 - sparse_categorical_accuracy: 0.9167\n",
      "Epoch 257/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 79us/sample - loss: 0.3603 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 258/500\n",
      "120/120 [==============================] - 0s 78us/sample - loss: 0.3593 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 259/500\n",
      "120/120 [==============================] - 0s 94us/sample - loss: 0.3550 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 260/500\n",
      "120/120 [==============================] - 0s 122us/sample - loss: 0.3702 - sparse_categorical_accuracy: 0.9333 - val_loss: 0.3728 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 261/500\n",
      "120/120 [==============================] - 0s 86us/sample - loss: 0.3443 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 262/500\n",
      "120/120 [==============================] - 0s 87us/sample - loss: 0.3642 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 263/500\n",
      "120/120 [==============================] - 0s 74us/sample - loss: 0.3592 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 264/500\n",
      "120/120 [==============================] - 0s 87us/sample - loss: 0.3484 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 265/500\n",
      "120/120 [==============================] - 0s 79us/sample - loss: 0.3374 - sparse_categorical_accuracy: 0.9333\n",
      "Epoch 266/500\n",
      "120/120 [==============================] - 0s 79us/sample - loss: 0.3543 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 267/500\n",
      "120/120 [==============================] - 0s 89us/sample - loss: 0.3952 - sparse_categorical_accuracy: 0.9333\n",
      "Epoch 268/500\n",
      "120/120 [==============================] - 0s 74us/sample - loss: 0.3469 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 269/500\n",
      "120/120 [==============================] - 0s 87us/sample - loss: 0.4562 - sparse_categorical_accuracy: 0.8417\n",
      "Epoch 270/500\n",
      "120/120 [==============================] - 0s 87us/sample - loss: 0.3673 - sparse_categorical_accuracy: 0.9083\n",
      "Epoch 271/500\n",
      "120/120 [==============================] - 0s 87us/sample - loss: 0.3751 - sparse_categorical_accuracy: 0.9167\n",
      "Epoch 272/500\n",
      "120/120 [==============================] - 0s 89us/sample - loss: 0.4066 - sparse_categorical_accuracy: 0.9083\n",
      "Epoch 273/500\n",
      "120/120 [==============================] - 0s 90us/sample - loss: 0.3520 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 274/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.3508 - sparse_categorical_accuracy: 0.9750\n",
      "Epoch 275/500\n",
      "120/120 [==============================] - 0s 90us/sample - loss: 0.4247 - sparse_categorical_accuracy: 0.8500\n",
      "Epoch 276/500\n",
      "120/120 [==============================] - 0s 86us/sample - loss: 0.5184 - sparse_categorical_accuracy: 0.8000\n",
      "Epoch 277/500\n",
      "120/120 [==============================] - 0s 88us/sample - loss: 0.3586 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 278/500\n",
      "120/120 [==============================] - 0s 87us/sample - loss: 0.4024 - sparse_categorical_accuracy: 0.8917\n",
      "Epoch 279/500\n",
      "120/120 [==============================] - 0s 87us/sample - loss: 0.3472 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 280/500\n",
      "120/120 [==============================] - 0s 131us/sample - loss: 0.3479 - sparse_categorical_accuracy: 0.9417 - val_loss: 0.5195 - val_sparse_categorical_accuracy: 0.6667\n",
      "Epoch 281/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.5396 - sparse_categorical_accuracy: 0.7750\n",
      "Epoch 282/500\n",
      "120/120 [==============================] - 0s 74us/sample - loss: 0.3576 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 283/500\n",
      "120/120 [==============================] - 0s 87us/sample - loss: 0.4066 - sparse_categorical_accuracy: 0.9083\n",
      "Epoch 284/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3588 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 285/500\n",
      "120/120 [==============================] - 0s 88us/sample - loss: 0.4913 - sparse_categorical_accuracy: 0.8417\n",
      "Epoch 286/500\n",
      "120/120 [==============================] - 0s 85us/sample - loss: 0.3891 - sparse_categorical_accuracy: 0.8833\n",
      "Epoch 287/500\n",
      "120/120 [==============================] - 0s 86us/sample - loss: 0.3614 - sparse_categorical_accuracy: 0.9750\n",
      "Epoch 288/500\n",
      "120/120 [==============================] - 0s 87us/sample - loss: 0.3402 - sparse_categorical_accuracy: 0.9833\n",
      "Epoch 289/500\n",
      "120/120 [==============================] - 0s 90us/sample - loss: 0.3814 - sparse_categorical_accuracy: 0.9167\n",
      "Epoch 290/500\n",
      "120/120 [==============================] - 0s 89us/sample - loss: 0.3901 - sparse_categorical_accuracy: 0.9000\n",
      "Epoch 291/500\n",
      "120/120 [==============================] - 0s 89us/sample - loss: 0.3999 - sparse_categorical_accuracy: 0.9000\n",
      "Epoch 292/500\n",
      "120/120 [==============================] - 0s 89us/sample - loss: 0.3704 - sparse_categorical_accuracy: 0.9333\n",
      "Epoch 293/500\n",
      "120/120 [==============================] - 0s 90us/sample - loss: 0.3493 - sparse_categorical_accuracy: 0.9333\n",
      "Epoch 294/500\n",
      "120/120 [==============================] - 0s 86us/sample - loss: 0.3948 - sparse_categorical_accuracy: 0.9333\n",
      "Epoch 295/500\n",
      "120/120 [==============================] - 0s 87us/sample - loss: 0.4169 - sparse_categorical_accuracy: 0.8667\n",
      "Epoch 296/500\n",
      "120/120 [==============================] - 0s 86us/sample - loss: 0.3815 - sparse_categorical_accuracy: 0.9333\n",
      "Epoch 297/500\n",
      "120/120 [==============================] - 0s 95us/sample - loss: 0.3795 - sparse_categorical_accuracy: 0.9083\n",
      "Epoch 298/500\n",
      "120/120 [==============================] - 0s 90us/sample - loss: 0.3618 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 299/500\n",
      "120/120 [==============================] - 0s 87us/sample - loss: 0.3718 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 300/500\n",
      "120/120 [==============================] - 0s 121us/sample - loss: 0.3682 - sparse_categorical_accuracy: 0.9333 - val_loss: 0.3514 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 301/500\n",
      "120/120 [==============================] - 0s 74us/sample - loss: 0.3612 - sparse_categorical_accuracy: 0.9000\n",
      "Epoch 302/500\n",
      "120/120 [==============================] - 0s 74us/sample - loss: 0.4212 - sparse_categorical_accuracy: 0.8667\n",
      "Epoch 303/500\n",
      "120/120 [==============================] - 0s 73us/sample - loss: 0.3701 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 304/500\n",
      "120/120 [==============================] - 0s 74us/sample - loss: 0.3714 - sparse_categorical_accuracy: 0.9167\n",
      "Epoch 305/500\n",
      "120/120 [==============================] - 0s 91us/sample - loss: 0.3543 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 306/500\n",
      "120/120 [==============================] - 0s 90us/sample - loss: 0.3445 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 307/500\n",
      "120/120 [==============================] - 0s 88us/sample - loss: 0.3452 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 308/500\n",
      "120/120 [==============================] - 0s 85us/sample - loss: 0.3441 - sparse_categorical_accuracy: 0.9833\n",
      "Epoch 309/500\n",
      "120/120 [==============================] - 0s 87us/sample - loss: 0.3593 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 310/500\n",
      "120/120 [==============================] - 0s 87us/sample - loss: 0.3656 - sparse_categorical_accuracy: 0.9167\n",
      "Epoch 311/500\n",
      "120/120 [==============================] - 0s 86us/sample - loss: 0.3575 - sparse_categorical_accuracy: 0.9333\n",
      "Epoch 312/500\n",
      "120/120 [==============================] - 0s 88us/sample - loss: 0.3515 - sparse_categorical_accuracy: 0.9750\n",
      "Epoch 313/500\n",
      "120/120 [==============================] - 0s 91us/sample - loss: 0.3453 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 314/500\n",
      "120/120 [==============================] - 0s 78us/sample - loss: 0.3416 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 315/500\n",
      "120/120 [==============================] - 0s 88us/sample - loss: 0.3368 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 316/500\n",
      "120/120 [==============================] - 0s 89us/sample - loss: 0.4223 - sparse_categorical_accuracy: 0.8917\n",
      "Epoch 317/500\n",
      "120/120 [==============================] - 0s 87us/sample - loss: 0.3694 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 318/500\n",
      "120/120 [==============================] - 0s 86us/sample - loss: 0.3409 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 319/500\n",
      "120/120 [==============================] - 0s 74us/sample - loss: 0.3643 - sparse_categorical_accuracy: 0.9333\n",
      "Epoch 320/500\n",
      "120/120 [==============================] - 0s 120us/sample - loss: 0.3563 - sparse_categorical_accuracy: 0.9500 - val_loss: 0.6103 - val_sparse_categorical_accuracy: 0.6000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 321/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3852 - sparse_categorical_accuracy: 0.9167\n",
      "Epoch 322/500\n",
      "120/120 [==============================] - 0s 88us/sample - loss: 0.3413 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 323/500\n",
      "120/120 [==============================] - 0s 89us/sample - loss: 0.3346 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 324/500\n",
      "120/120 [==============================] - 0s 87us/sample - loss: 0.3535 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 325/500\n",
      "120/120 [==============================] - 0s 87us/sample - loss: 0.3591 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 326/500\n",
      "120/120 [==============================] - 0s 86us/sample - loss: 0.3533 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 327/500\n",
      "120/120 [==============================] - 0s 86us/sample - loss: 0.3399 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 328/500\n",
      "120/120 [==============================] - 0s 85us/sample - loss: 0.4023 - sparse_categorical_accuracy: 0.9000\n",
      "Epoch 329/500\n",
      "120/120 [==============================] - 0s 86us/sample - loss: 0.3758 - sparse_categorical_accuracy: 0.9167\n",
      "Epoch 330/500\n",
      "120/120 [==============================] - 0s 85us/sample - loss: 0.3763 - sparse_categorical_accuracy: 0.9333\n",
      "Epoch 331/500\n",
      "120/120 [==============================] - 0s 84us/sample - loss: 0.3627 - sparse_categorical_accuracy: 0.9000\n",
      "Epoch 332/500\n",
      "120/120 [==============================] - 0s 73us/sample - loss: 0.3408 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 333/500\n",
      "120/120 [==============================] - 0s 84us/sample - loss: 0.3571 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 334/500\n",
      "120/120 [==============================] - 0s 71us/sample - loss: 0.3856 - sparse_categorical_accuracy: 0.9167\n",
      "Epoch 335/500\n",
      "120/120 [==============================] - 0s 84us/sample - loss: 0.4155 - sparse_categorical_accuracy: 0.8750\n",
      "Epoch 336/500\n",
      "120/120 [==============================] - 0s 71us/sample - loss: 0.5168 - sparse_categorical_accuracy: 0.7917\n",
      "Epoch 337/500\n",
      "120/120 [==============================] - 0s 74us/sample - loss: 0.3682 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 338/500\n",
      "120/120 [==============================] - 0s 73us/sample - loss: 0.3524 - sparse_categorical_accuracy: 0.9750\n",
      "Epoch 339/500\n",
      "120/120 [==============================] - 0s 85us/sample - loss: 0.3811 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 340/500\n",
      "120/120 [==============================] - 0s 129us/sample - loss: 0.3978 - sparse_categorical_accuracy: 0.8833 - val_loss: 0.4761 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 341/500\n",
      "120/120 [==============================] - 0s 71us/sample - loss: 0.3589 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 342/500\n",
      "120/120 [==============================] - 0s 72us/sample - loss: 0.3607 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 343/500\n",
      "120/120 [==============================] - 0s 85us/sample - loss: 0.3863 - sparse_categorical_accuracy: 0.9083\n",
      "Epoch 344/500\n",
      "120/120 [==============================] - 0s 84us/sample - loss: 0.3998 - sparse_categorical_accuracy: 0.9000\n",
      "Epoch 345/500\n",
      "120/120 [==============================] - 0s 84us/sample - loss: 0.3920 - sparse_categorical_accuracy: 0.9083\n",
      "Epoch 346/500\n",
      "120/120 [==============================] - 0s 84us/sample - loss: 0.4703 - sparse_categorical_accuracy: 0.8667\n",
      "Epoch 347/500\n",
      "120/120 [==============================] - 0s 87us/sample - loss: 0.4195 - sparse_categorical_accuracy: 0.8833\n",
      "Epoch 348/500\n",
      "120/120 [==============================] - 0s 87us/sample - loss: 0.3537 - sparse_categorical_accuracy: 0.9333\n",
      "Epoch 349/500\n",
      "120/120 [==============================] - 0s 73us/sample - loss: 0.3494 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 350/500\n",
      "120/120 [==============================] - 0s 85us/sample - loss: 0.3398 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 351/500\n",
      "120/120 [==============================] - 0s 72us/sample - loss: 0.3429 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 352/500\n",
      "120/120 [==============================] - 0s 85us/sample - loss: 0.3463 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 353/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3714 - sparse_categorical_accuracy: 0.9167\n",
      "Epoch 354/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.3735 - sparse_categorical_accuracy: 0.9000\n",
      "Epoch 355/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3949 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 356/500\n",
      "120/120 [==============================] - 0s 73us/sample - loss: 0.3822 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 357/500\n",
      "120/120 [==============================] - 0s 85us/sample - loss: 0.3570 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 358/500\n",
      "120/120 [==============================] - 0s 73us/sample - loss: 0.4167 - sparse_categorical_accuracy: 0.8750\n",
      "Epoch 359/500\n",
      "120/120 [==============================] - 0s 73us/sample - loss: 0.4038 - sparse_categorical_accuracy: 0.8583\n",
      "Epoch 360/500\n",
      "120/120 [==============================] - 0s 118us/sample - loss: 0.3636 - sparse_categorical_accuracy: 0.9417 - val_loss: 0.8138 - val_sparse_categorical_accuracy: 0.5333\n",
      "Epoch 361/500\n",
      "120/120 [==============================] - 0s 83us/sample - loss: 0.4337 - sparse_categorical_accuracy: 0.8667\n",
      "Epoch 362/500\n",
      "120/120 [==============================] - 0s 89us/sample - loss: 0.3853 - sparse_categorical_accuracy: 0.9167\n",
      "Epoch 363/500\n",
      "120/120 [==============================] - 0s 86us/sample - loss: 0.3598 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 364/500\n",
      "120/120 [==============================] - 0s 85us/sample - loss: 0.3588 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 365/500\n",
      "120/120 [==============================] - 0s 84us/sample - loss: 0.4300 - sparse_categorical_accuracy: 0.8667\n",
      "Epoch 366/500\n",
      "120/120 [==============================] - 0s 73us/sample - loss: 0.3912 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 367/500\n",
      "120/120 [==============================] - 0s 84us/sample - loss: 0.3395 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 368/500\n",
      "120/120 [==============================] - 0s 73us/sample - loss: 0.4010 - sparse_categorical_accuracy: 0.8833\n",
      "Epoch 369/500\n",
      "120/120 [==============================] - 0s 84us/sample - loss: 0.3484 - sparse_categorical_accuracy: 0.9333\n",
      "Epoch 370/500\n",
      "120/120 [==============================] - 0s 72us/sample - loss: 0.3394 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 371/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3912 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 372/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3630 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 373/500\n",
      "120/120 [==============================] - 0s 73us/sample - loss: 0.3698 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 374/500\n",
      "120/120 [==============================] - 0s 85us/sample - loss: 0.3446 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 375/500\n",
      "120/120 [==============================] - 0s 73us/sample - loss: 0.3443 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 376/500\n",
      "120/120 [==============================] - 0s 73us/sample - loss: 0.4100 - sparse_categorical_accuracy: 0.9000\n",
      "Epoch 377/500\n",
      "120/120 [==============================] - 0s 73us/sample - loss: 0.3428 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 378/500\n",
      "120/120 [==============================] - 0s 86us/sample - loss: 0.3732 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 379/500\n",
      "120/120 [==============================] - 0s 85us/sample - loss: 0.4839 - sparse_categorical_accuracy: 0.7917\n",
      "Epoch 380/500\n",
      "120/120 [==============================] - 0s 137us/sample - loss: 0.3773 - sparse_categorical_accuracy: 0.9083 - val_loss: 0.3185 - val_sparse_categorical_accuracy: 0.9667\n",
      "Epoch 381/500\n",
      "120/120 [==============================] - 0s 90us/sample - loss: 0.3405 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 382/500\n",
      "120/120 [==============================] - 0s 96us/sample - loss: 0.3427 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 383/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3512 - sparse_categorical_accuracy: 0.9333\n",
      "Epoch 384/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.3712 - sparse_categorical_accuracy: 0.9083\n",
      "Epoch 385/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 74us/sample - loss: 0.3541 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 386/500\n",
      "120/120 [==============================] - 0s 73us/sample - loss: 0.4142 - sparse_categorical_accuracy: 0.8833\n",
      "Epoch 387/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3379 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 388/500\n",
      "120/120 [==============================] - 0s 79us/sample - loss: 0.3353 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 389/500\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.3579 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 390/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3770 - sparse_categorical_accuracy: 0.9167\n",
      "Epoch 391/500\n",
      "120/120 [==============================] - 0s 73us/sample - loss: 0.3374 - sparse_categorical_accuracy: 0.9750\n",
      "Epoch 392/500\n",
      "120/120 [==============================] - 0s 72us/sample - loss: 0.3372 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 393/500\n",
      "120/120 [==============================] - 0s 71us/sample - loss: 0.3448 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 394/500\n",
      "120/120 [==============================] - 0s 73us/sample - loss: 0.3763 - sparse_categorical_accuracy: 0.9167\n",
      "Epoch 395/500\n",
      "120/120 [==============================] - 0s 86us/sample - loss: 0.3595 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 396/500\n",
      "120/120 [==============================] - 0s 87us/sample - loss: 0.3794 - sparse_categorical_accuracy: 0.9167\n",
      "Epoch 397/500\n",
      "120/120 [==============================] - 0s 85us/sample - loss: 0.3636 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 398/500\n",
      "120/120 [==============================] - 0s 85us/sample - loss: 0.3539 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 399/500\n",
      "120/120 [==============================] - 0s 72us/sample - loss: 0.3476 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 400/500\n",
      "120/120 [==============================] - 0s 117us/sample - loss: 0.3774 - sparse_categorical_accuracy: 0.9167 - val_loss: 0.3298 - val_sparse_categorical_accuracy: 0.9667\n",
      "Epoch 401/500\n",
      "120/120 [==============================] - 0s 82us/sample - loss: 0.3674 - sparse_categorical_accuracy: 0.9000\n",
      "Epoch 402/500\n",
      "120/120 [==============================] - 0s 84us/sample - loss: 0.4001 - sparse_categorical_accuracy: 0.9167\n",
      "Epoch 403/500\n",
      "120/120 [==============================] - 0s 72us/sample - loss: 0.3456 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 404/500\n",
      "120/120 [==============================] - 0s 73us/sample - loss: 0.3913 - sparse_categorical_accuracy: 0.8667\n",
      "Epoch 405/500\n",
      "120/120 [==============================] - 0s 74us/sample - loss: 0.3413 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 406/500\n",
      "120/120 [==============================] - 0s 86us/sample - loss: 0.3499 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 407/500\n",
      "120/120 [==============================] - 0s 72us/sample - loss: 0.4235 - sparse_categorical_accuracy: 0.8833\n",
      "Epoch 408/500\n",
      "120/120 [==============================] - 0s 72us/sample - loss: 0.3572 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 409/500\n",
      "120/120 [==============================] - 0s 72us/sample - loss: 0.3497 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 410/500\n",
      "120/120 [==============================] - 0s 72us/sample - loss: 0.3387 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 411/500\n",
      "120/120 [==============================] - 0s 72us/sample - loss: 0.3618 - sparse_categorical_accuracy: 0.9167\n",
      "Epoch 412/500\n",
      "120/120 [==============================] - 0s 72us/sample - loss: 0.3569 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 413/500\n",
      "120/120 [==============================] - 0s 86us/sample - loss: 0.3722 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 414/500\n",
      "120/120 [==============================] - 0s 89us/sample - loss: 0.3410 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 415/500\n",
      "120/120 [==============================] - 0s 85us/sample - loss: 0.3571 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 416/500\n",
      "120/120 [==============================] - 0s 84us/sample - loss: 0.3866 - sparse_categorical_accuracy: 0.9167\n",
      "Epoch 417/500\n",
      "120/120 [==============================] - 0s 74us/sample - loss: 0.3416 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 418/500\n",
      "120/120 [==============================] - 0s 72us/sample - loss: 0.3459 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 419/500\n",
      "120/120 [==============================] - 0s 71us/sample - loss: 0.3700 - sparse_categorical_accuracy: 0.9167\n",
      "Epoch 420/500\n",
      "120/120 [==============================] - 0s 128us/sample - loss: 0.4336 - sparse_categorical_accuracy: 0.8917 - val_loss: 0.6770 - val_sparse_categorical_accuracy: 0.6000\n",
      "Epoch 421/500\n",
      "120/120 [==============================] - 0s 83us/sample - loss: 0.4614 - sparse_categorical_accuracy: 0.8333\n",
      "Epoch 422/500\n",
      "120/120 [==============================] - 0s 73us/sample - loss: 0.3684 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 423/500\n",
      "120/120 [==============================] - 0s 73us/sample - loss: 0.3927 - sparse_categorical_accuracy: 0.8750\n",
      "Epoch 424/500\n",
      "120/120 [==============================] - 0s 85us/sample - loss: 0.3352 - sparse_categorical_accuracy: 0.9833\n",
      "Epoch 425/500\n",
      "120/120 [==============================] - 0s 72us/sample - loss: 0.3867 - sparse_categorical_accuracy: 0.8833\n",
      "Epoch 426/500\n",
      "120/120 [==============================] - 0s 73us/sample - loss: 0.3432 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 427/500\n",
      "120/120 [==============================] - 0s 71us/sample - loss: 0.3410 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 428/500\n",
      "120/120 [==============================] - 0s 73us/sample - loss: 0.3425 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 429/500\n",
      "120/120 [==============================] - 0s 72us/sample - loss: 0.3393 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 430/500\n",
      "120/120 [==============================] - 0s 73us/sample - loss: 0.3373 - sparse_categorical_accuracy: 0.9750\n",
      "Epoch 431/500\n",
      "120/120 [==============================] - 0s 89us/sample - loss: 0.3624 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 432/500\n",
      "120/120 [==============================] - 0s 85us/sample - loss: 0.3596 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 433/500\n",
      "120/120 [==============================] - 0s 85us/sample - loss: 0.3558 - sparse_categorical_accuracy: 0.9333\n",
      "Epoch 434/500\n",
      "120/120 [==============================] - 0s 84us/sample - loss: 0.3538 - sparse_categorical_accuracy: 0.9333\n",
      "Epoch 435/500\n",
      "120/120 [==============================] - 0s 73us/sample - loss: 0.3960 - sparse_categorical_accuracy: 0.8833\n",
      "Epoch 436/500\n",
      "120/120 [==============================] - 0s 71us/sample - loss: 0.3635 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 437/500\n",
      "120/120 [==============================] - 0s 73us/sample - loss: 0.4245 - sparse_categorical_accuracy: 0.8583\n",
      "Epoch 438/500\n",
      "120/120 [==============================] - 0s 73us/sample - loss: 0.3392 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 439/500\n",
      "120/120 [==============================] - 0s 85us/sample - loss: 0.3492 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 440/500\n",
      "120/120 [==============================] - 0s 123us/sample - loss: 0.3371 - sparse_categorical_accuracy: 0.9583 - val_loss: 0.3438 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 441/500\n",
      "120/120 [==============================] - 0s 72us/sample - loss: 0.3701 - sparse_categorical_accuracy: 0.9167\n",
      "Epoch 442/500\n",
      "120/120 [==============================] - 0s 84us/sample - loss: 0.3417 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 443/500\n",
      "120/120 [==============================] - 0s 72us/sample - loss: 0.3538 - sparse_categorical_accuracy: 0.9333\n",
      "Epoch 444/500\n",
      "120/120 [==============================] - 0s 72us/sample - loss: 0.3401 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 445/500\n",
      "120/120 [==============================] - 0s 72us/sample - loss: 0.3775 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 446/500\n",
      "120/120 [==============================] - 0s 72us/sample - loss: 0.3463 - sparse_categorical_accuracy: 0.9333\n",
      "Epoch 447/500\n",
      "120/120 [==============================] - 0s 72us/sample - loss: 0.4087 - sparse_categorical_accuracy: 0.9000\n",
      "Epoch 448/500\n",
      "120/120 [==============================] - 0s 74us/sample - loss: 0.3768 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 449/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 86us/sample - loss: 0.3658 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 450/500\n",
      "120/120 [==============================] - 0s 85us/sample - loss: 0.3906 - sparse_categorical_accuracy: 0.9167\n",
      "Epoch 451/500\n",
      "120/120 [==============================] - 0s 85us/sample - loss: 0.3470 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 452/500\n",
      "120/120 [==============================] - 0s 84us/sample - loss: 0.3467 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 453/500\n",
      "120/120 [==============================] - 0s 85us/sample - loss: 0.3642 - sparse_categorical_accuracy: 0.9083\n",
      "Epoch 454/500\n",
      "120/120 [==============================] - 0s 72us/sample - loss: 0.3548 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 455/500\n",
      "120/120 [==============================] - 0s 72us/sample - loss: 0.3693 - sparse_categorical_accuracy: 0.9167\n",
      "Epoch 456/500\n",
      "120/120 [==============================] - 0s 73us/sample - loss: 0.3876 - sparse_categorical_accuracy: 0.9083\n",
      "Epoch 457/500\n",
      "120/120 [==============================] - 0s 88us/sample - loss: 0.3648 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 458/500\n",
      "120/120 [==============================] - 0s 73us/sample - loss: 0.4028 - sparse_categorical_accuracy: 0.8833\n",
      "Epoch 459/500\n",
      "120/120 [==============================] - 0s 73us/sample - loss: 0.3596 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 460/500\n",
      "120/120 [==============================] - 0s 115us/sample - loss: 0.4119 - sparse_categorical_accuracy: 0.8833 - val_loss: 0.5965 - val_sparse_categorical_accuracy: 0.6000\n",
      "Epoch 461/500\n",
      "120/120 [==============================] - 0s 84us/sample - loss: 0.3860 - sparse_categorical_accuracy: 0.8833\n",
      "Epoch 462/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.4139 - sparse_categorical_accuracy: 0.8750\n",
      "Epoch 463/500\n",
      "120/120 [==============================] - 0s 85us/sample - loss: 0.3460 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 464/500\n",
      "120/120 [==============================] - 0s 86us/sample - loss: 0.3623 - sparse_categorical_accuracy: 0.9333\n",
      "Epoch 465/500\n",
      "120/120 [==============================] - 0s 79us/sample - loss: 0.3343 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 466/500\n",
      "120/120 [==============================] - 0s 85us/sample - loss: 0.3910 - sparse_categorical_accuracy: 0.9083\n",
      "Epoch 467/500\n",
      "120/120 [==============================] - 0s 84us/sample - loss: 0.4764 - sparse_categorical_accuracy: 0.8250\n",
      "Epoch 468/500\n",
      "120/120 [==============================] - 0s 86us/sample - loss: 0.3636 - sparse_categorical_accuracy: 0.9250\n",
      "Epoch 469/500\n",
      "120/120 [==============================] - 0s 73us/sample - loss: 0.3425 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 470/500\n",
      "120/120 [==============================] - 0s 84us/sample - loss: 0.3753 - sparse_categorical_accuracy: 0.9333\n",
      "Epoch 471/500\n",
      "120/120 [==============================] - 0s 74us/sample - loss: 0.4553 - sparse_categorical_accuracy: 0.8167\n",
      "Epoch 472/500\n",
      "120/120 [==============================] - 0s 73us/sample - loss: 0.3502 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 473/500\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.3874 - sparse_categorical_accuracy: 0.9000\n",
      "Epoch 474/500\n",
      "120/120 [==============================] - 0s 90us/sample - loss: 0.4106 - sparse_categorical_accuracy: 0.8667\n",
      "Epoch 475/500\n",
      "120/120 [==============================] - 0s 74us/sample - loss: 0.3458 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 476/500\n",
      "120/120 [==============================] - 0s 73us/sample - loss: 0.3452 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 477/500\n",
      "120/120 [==============================] - 0s 71us/sample - loss: 0.3426 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 478/500\n",
      "120/120 [==============================] - 0s 74us/sample - loss: 0.3489 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 479/500\n",
      "120/120 [==============================] - 0s 83us/sample - loss: 0.3354 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 480/500\n",
      "120/120 [==============================] - 0s 129us/sample - loss: 0.3357 - sparse_categorical_accuracy: 0.9833 - val_loss: 0.3341 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 481/500\n",
      "120/120 [==============================] - 0s 86us/sample - loss: 0.3943 - sparse_categorical_accuracy: 0.9000\n",
      "Epoch 482/500\n",
      "120/120 [==============================] - 0s 75us/sample - loss: 0.3412 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 483/500\n",
      "120/120 [==============================] - 0s 84us/sample - loss: 0.3632 - sparse_categorical_accuracy: 0.9333\n",
      "Epoch 484/500\n",
      "120/120 [==============================] - 0s 83us/sample - loss: 0.3523 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 485/500\n",
      "120/120 [==============================] - 0s 86us/sample - loss: 0.3508 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 486/500\n",
      "120/120 [==============================] - 0s 72us/sample - loss: 0.3649 - sparse_categorical_accuracy: 0.9167\n",
      "Epoch 487/500\n",
      "120/120 [==============================] - 0s 83us/sample - loss: 0.3386 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 488/500\n",
      "120/120 [==============================] - 0s 72us/sample - loss: 0.3598 - sparse_categorical_accuracy: 0.9333\n",
      "Epoch 489/500\n",
      "120/120 [==============================] - 0s 74us/sample - loss: 0.4113 - sparse_categorical_accuracy: 0.8667\n",
      "Epoch 490/500\n",
      "120/120 [==============================] - 0s 74us/sample - loss: 0.4515 - sparse_categorical_accuracy: 0.8500\n",
      "Epoch 491/500\n",
      "120/120 [==============================] - 0s 88us/sample - loss: 0.4595 - sparse_categorical_accuracy: 0.8500\n",
      "Epoch 492/500\n",
      "120/120 [==============================] - 0s 85us/sample - loss: 0.3822 - sparse_categorical_accuracy: 0.9083\n",
      "Epoch 493/500\n",
      "120/120 [==============================] - 0s 71us/sample - loss: 0.3504 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 494/500\n",
      "120/120 [==============================] - 0s 73us/sample - loss: 0.3589 - sparse_categorical_accuracy: 0.9083\n",
      "Epoch 495/500\n",
      "120/120 [==============================] - 0s 73us/sample - loss: 0.5808 - sparse_categorical_accuracy: 0.7333\n",
      "Epoch 496/500\n",
      "120/120 [==============================] - 0s 83us/sample - loss: 0.3468 - sparse_categorical_accuracy: 0.9417\n",
      "Epoch 497/500\n",
      "120/120 [==============================] - 0s 85us/sample - loss: 0.3640 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 498/500\n",
      "120/120 [==============================] - 0s 85us/sample - loss: 0.3336 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 499/500\n",
      "120/120 [==============================] - 0s 85us/sample - loss: 0.3486 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 500/500\n",
      "120/120 [==============================] - 0s 126us/sample - loss: 0.3333 - sparse_categorical_accuracy: 0.9667 - val_loss: 0.4002 - val_sparse_categorical_accuracy: 1.0000\n",
      "Model: \"iris_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              multiple                  15        \n",
      "=================================================================\n",
      "Total params: 15\n",
      "Trainable params: 15\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import Model\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "\n",
    "x_train = datasets.load_iris().data\n",
    "y_train = datasets.load_iris().target\n",
    "\n",
    "np.random.seed(116)\n",
    "np.random.shuffle(x_train)\n",
    "np.random.seed(116)\n",
    "np.random.shuffle(y_train)\n",
    "tf.random.set_seed(116)\n",
    "\n",
    "class IrisModel(Model):\n",
    "    def __init__(self):\n",
    "        super(IrisModel, self).__init__()\n",
    "        self.d1 = Dense(3, activation='softmax', kernel_regularizer=tf.keras.regularizers.l2())\n",
    "\n",
    "    def call(self, x):\n",
    "        y = self.d1(x)\n",
    "        return y\n",
    "\n",
    "model = IrisModel()\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(lr=0.1),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "              metrics=['sparse_categorical_accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=500, validation_split=0.2, validation_freq=20)\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
