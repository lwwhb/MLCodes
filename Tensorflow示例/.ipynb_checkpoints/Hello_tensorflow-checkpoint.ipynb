{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 深度学习与机器学习的区别\n",
    "   * 特征提取方面 ： 机器学习有人工特征提取步骤，深度学习没有\n",
    "   * 数据量与计算性能要求方面 ： 机器学习执行时间往往少于深度学习，深度学习参数往往很庞大，需要通过大量数据多次优化来训练参数\n",
    "   * 算法代表方面：机器学习通常为朴素贝叶斯、决策树等；深度学习通过神经网络。\n",
    "   \n",
    "### 2. Tensorflow 2.0\n",
    "#### 2.1 张量\n",
    " * 张量（Tensor）:多为数组（列表） 阶：张量的维数\n",
    " \n",
    "维数 | 阶 | 名字 | 例子 \n",
    ":-: | :-: | :-: | :-: \n",
    "0-D | 0 | 标量 scalar | s=1 2 3 \n",
    "1-D | 1 | 向量 vector | v=[1,2,3]\n",
    "2-D | 2 | 矩阵 matrix | m=[[1,2,3],[4,5,6],[7,8,9]]\n",
    "n-D | 3 | 张量 tensor | t=[[[ n个\n",
    "     \n",
    " * 数据类型：\n",
    "    - tf.int, tf.float.....\n",
    "        - tf.int32, tf.float32, tf.float64\n",
    "    - tf.bool\n",
    "        - tf.constant([True,False])\n",
    "    - tf.string\n",
    "        - tf.constant(\"hello,world!\")\n",
    " * 创建张量\n",
    "    - tf.constant(张量内容，dtype=数据类型（可选）)\n",
    "    - tf.zeros(维度)\n",
    "    - tf.ones(维度)\n",
    "    - tf.fill(维度，指定值)\n",
    " * Numpy类型转tensor类型\n",
    "    - tf.convert_to_tensor(数据名，dtype=数据类型（可选）)\n",
    " * 生成正态分布的随机数，默认均值为0，标准差为1\n",
    "    - tf.random.normal(维度，mean=均值，stddev=标准差)\n",
    " * 生成截断式正太分布随机数\n",
    "    - tf.random.truncated_normal(维度，mean=均值，stddev=标准差)  （数据在两个标准差之内）\n",
    " * 生成均匀分布随机数 [minval, maxval]\n",
    "    - tf.random.uniform(维度，minval=最小值，maxval=最大值)\n",
    "    \n",
    " #### 2.2 常用函数\n",
    " * 强制tensor转为为该数据类型\n",
    "    - tf.cast(张量名，dtype=数据类型)\n",
    " * 计算张量维度上元素的最小值\n",
    "    - tf.reduce_min(张量名)\n",
    " * 计算张量维度上元素的最大值\n",
    "    - tf.reduce_max(张量名)\n",
    " * 理解axis\n",
    "    - axis=0代表跨行（经度），axis=1代表跨列（纬度）\n",
    " * 计算张量沿着指定维度的平均值\n",
    "    - tf.reduce_mean(张量名，axis=操作轴)\n",
    " * 计算张量沿着指定维度的和\n",
    "    - tf.reduce_sum(张量名，axis=操作轴)\n",
    " * tf.Variable()将变量标记为“可训练的”\n",
    "    - 被标记的变量会在反向传播中记录梯度信息。在神经网络训练中，常用该函数标记待训练参数。tf.Variable(初始值)\n",
    " * 数学运算\n",
    "    - 对应元素四则运算(只有维度相同的张量才能进行四则运算)：tf.add,tf.subtract,tf.multiply,tf.divide\n",
    "    - 平方、次方与开方：tf.square,tf.pow,tf.sqrt\n",
    "    - 矩阵乘法：tf.matmul\n",
    " * 切分传入张量的第一维度，生成输入特征/标签对，构建数据集(Numpy与Tensor格式都可以用该语句读入数据)\n",
    "    - data=tf.data.Dataset.from_tensor_slices((输入特征，标签))\n",
    " * tf.GradientTape 实现某个函数对指定参数的求导运算\n",
    "    - with结构记录计算过程，gradient求出张量的梯度 \n",
    "        with tf.GradientTape() as tape:\n",
    "            grad=tape.gradient(函数，对谁求导)\n",
    " * enumerate(列表名)\n",
    "    - enumerate是python的内建函数，它可遍历每个元素（如列表、元组或字符串），组合为：索引 元素，常在for循环中使用\n",
    " * tf.one_hot(待转换数据，depth=几分类) 函数将待转换数据，转换为one-hot形式的数据输出\n",
    "    - 独热编码（ one-hot encoding ）: 在分类问题中，常用独热码做标签，标记类别：1表示是，0表示非\n",
    " * tf.nn.softmax(x) 当n分类的n个输出（y0,y1,....yn-1）通过softmax（）函数 使输出符合概率分布  \n",
    "    - $Softmax({y_i})=\\frac {e^{y_i}}{\\sum_{j=0}^ne^{y_i}}$\n",
    " * assign_sub函数常用于函数的自更新\n",
    "    - 赋值操作，更新参数的值并返回\n",
    "    - 调用assign_sub前，先用tf.Variable定义变量w为可训练（可自更新）\n",
    "    - w.assign_sub(w要减的内容) \n",
    " * tf.argmax 返回张量沿指定维度最大值的索引\n",
    "    - tf argmax(张量名，axis=操作轴)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 epoch, w is 2.600000, loss is 36.000000\n",
      "After 1 epoch, w is 1.160000, loss is 12.959999\n",
      "After 2 epoch, w is 0.296000, loss is 4.665599\n",
      "After 3 epoch, w is -0.222400, loss is 1.679616\n",
      "After 4 epoch, w is -0.533440, loss is 0.604662\n",
      "After 5 epoch, w is -0.720064, loss is 0.217678\n",
      "After 6 epoch, w is -0.832038, loss is 0.078364\n",
      "After 7 epoch, w is -0.899223, loss is 0.028211\n",
      "After 8 epoch, w is -0.939534, loss is 0.010156\n",
      "After 9 epoch, w is -0.963720, loss is 0.003656\n",
      "After 10 epoch, w is -0.978232, loss is 0.001316\n",
      "After 11 epoch, w is -0.986939, loss is 0.000474\n",
      "After 12 epoch, w is -0.992164, loss is 0.000171\n",
      "After 13 epoch, w is -0.995298, loss is 0.000061\n",
      "After 14 epoch, w is -0.997179, loss is 0.000022\n",
      "After 15 epoch, w is -0.998307, loss is 0.000008\n",
      "After 16 epoch, w is -0.998984, loss is 0.000003\n",
      "After 17 epoch, w is -0.999391, loss is 0.000001\n",
      "After 18 epoch, w is -0.999634, loss is 0.000000\n",
      "After 19 epoch, w is -0.999781, loss is 0.000000\n",
      "After 20 epoch, w is -0.999868, loss is 0.000000\n",
      "After 21 epoch, w is -0.999921, loss is 0.000000\n",
      "After 22 epoch, w is -0.999953, loss is 0.000000\n",
      "After 23 epoch, w is -0.999972, loss is 0.000000\n",
      "After 24 epoch, w is -0.999983, loss is 0.000000\n",
      "After 25 epoch, w is -0.999990, loss is 0.000000\n",
      "After 26 epoch, w is -0.999994, loss is 0.000000\n",
      "After 27 epoch, w is -0.999996, loss is 0.000000\n",
      "After 28 epoch, w is -0.999998, loss is 0.000000\n",
      "After 29 epoch, w is -0.999999, loss is 0.000000\n",
      "After 30 epoch, w is -0.999999, loss is 0.000000\n",
      "After 31 epoch, w is -1.000000, loss is 0.000000\n",
      "After 32 epoch, w is -1.000000, loss is 0.000000\n",
      "After 33 epoch, w is -1.000000, loss is 0.000000\n",
      "After 34 epoch, w is -1.000000, loss is 0.000000\n",
      "After 35 epoch, w is -1.000000, loss is 0.000000\n",
      "After 36 epoch, w is -1.000000, loss is 0.000000\n",
      "After 37 epoch, w is -1.000000, loss is 0.000000\n",
      "After 38 epoch, w is -1.000000, loss is 0.000000\n",
      "After 39 epoch, w is -1.000000, loss is 0.000000\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "def tensorflow_test():\n",
    "    #初始权重\n",
    "    w = tf.Variable(tf.constant(5, dtype=tf.float32))\n",
    "    #学习旅\n",
    "    lr = 0.2\n",
    "    #迭代次数\n",
    "    epoch = 40\n",
    "    # for epoch定义顶层循环，表示对数据机epoch次\n",
    "    for epoch in range(epoch): \n",
    "        # with结构到grads框起梯度计算过程\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = tf.square(w+1)\n",
    "        # .gradient函数告知谁对谁求导\n",
    "        grads = tape.gradient(loss,w)\n",
    "        # .assign_sub 对变量做自减 即：w-= lr*grads 即:w = w-lr*grads\n",
    "        w.assign_sub(lr*grads) \n",
    "        print(\"After %s epoch, w is %f, loss is %f\"%(epoch,w.numpy(),loss))\n",
    "    return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tensorflow_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 深度学习知识\n",
    "#### 3.1 预备知识\n",
    "* tf.where(条件语句，真返回A,假返回B): 条件语句真返回A,条件语句假返回B\n",
    "* np.random.RandomState(种子).rand(维度):返回一个[0,1) 之间的随机数\n",
    "* np.vStack(数组1，数组2)： 将两个数组按垂直方向叠加。\n",
    "* np.mgrid[起始值：结束值：步长，起始值：结束值：步长，...] \n",
    "* x.ravel()： 将x变成一维数组，“把.前变量拉直”\n",
    "* np.c_[数组1，数组2，...]：使返回的间隔数值点配对\n",
    "\n",
    "#### 3.2 神经网络复杂度\n",
    "* NN复杂度：多用NN层数和NN参数的个数表示\n",
    "    - 空间复杂度：\n",
    "        - 层数=隐藏层层数+1个输出层\n",
    "        - 总参数 = 总w+总b\n",
    "    - 时间复杂度：\n",
    "        - 乘法运算次数\n",
    "* 学习率：lr\n",
    "\n",
    "#### 3.3 指数衰减学习率\n",
    "* 指数衰减学习率，可以先用较大的学习旅，快速找到较优解，然后逐步减小学习旅，使模型在训练后期稳定。\n",
    "    -$指数衰减学习率= 初始学习旅*学习旅衰减率^{(当前轮数/多少轮衰减一次)}$\n",
    "    \n",
    "#### 3.4 激活函数\n",
    "* y=x*w+b是输入x的线性组合，而y=f(x*w+b)是1943年提出的MP模型，其中f是非线性函数，它的加入大大提升了模型的表达力。这使得多层神经网络不再是输入x的线性组合。神经网络可以随层数增加提升表达力了。\n",
    "* 优秀的激活函数：\n",
    "    - 非线性：激活函数非线性时，多层神经网络可逼近所有函数\n",
    "    - 可微性：优化器大多用梯度下降更新参数\n",
    "    - 单调性：当激活函数是单调的，能保证单层网络的损失函数是凸函数\n",
    "    - 近似恒等性：f(x)≈ x 当参数初始化位随机最小值时，神经网络更稳定\n",
    "* 激活函数输出值范围\n",
    "    - 激活函数输出为有限值时，给予梯度的优化方法更稳定\n",
    "    - 激活函数输出为无限值时，建议调小学习率\n",
    "* 激活函数：\n",
    "    - Sigmodi函数  tf.nn.sigmoid(x)  $f(x)=\\frac{1}{1+e^{-x}}$  \n",
    "        - 特点：\n",
    "            - 1.易造成梯度消失\n",
    "            - 2.输出非0均值，收敛慢\n",
    "            - 3.幂运算复杂，训练时间长\n",
    "    - Tanh函数 tf.nn.tanh(x)  $f(x)=\\frac{1-e^{-2x}}{1+e^{-2x}}$\n",
    "        - 特点：\n",
    "            - 1. 输出是0均值\n",
    "            - 2. 易造成梯度消失\n",
    "            - 3. 幂运算复杂，训练时间长\n",
    "    - Relu函数 tf.nn.relu(x)  $f(x)=max(x,0)$ x<0,f(x)=0; x>=0,f(x)=x\n",
    "        - 优点：\n",
    "            - 1. 解决了梯度消失问题（在正区间）\n",
    "            - 2. 只需判断输入是否大于0，计算速度快\n",
    "            - 3. 收敛速度远快于sigmoid和tanh\n",
    "        - 缺点：\n",
    "            - 1. 输出非0均值，收敛慢\n",
    "            - 2. Dead ReIU问题： 某些神经元可能永远无法被激活（送入激活函数的输入特征为负时），导致相应的参数永远不能被更新。\n",
    "    - Leaky Relu函数 tf.nn.leaky_relu(x) $f(x)=max(ax,x)$\n",
    "        * 理论上讲，Leaky Relu的Relu的所有优点，外加不会有Dead Relu问题，但在实际操作当中，并没有完全证明Leaky Relu总时好于Relu\n",
    "* 激活函数总结：对于初学者的建议：\n",
    "    - 首选relu激活函数；\n",
    "    - 学习率设置较小值；\n",
    "    - 输入特征标准化，即让输入特征满足以0为均值，1为标准差的正态分布\n",
    "    - 初始参数中心化，即让随机生成的参数满足以0为均值，$\\sqrt{\\frac{2}{当前层输入特征个数}}$ 为标准差的正态分布\n",
    "    \n",
    "#### 3.5 损失函数\n",
    "* 损失函数（loss）:预测值（y）与已知答案(y')的差距\n",
    "    NN优化目标：loss最小——> 1. mse(Mean squared error);2.自定义；3.ce(Cross Entropy) 交叉熵\n",
    "* 均方误差mse: $MSE(y,y')= \\frac{\\sum_{i=1}^n{(y-y')^2}}{n}$ , loss_mse = tf.reduce_mean(tf.square(y-y'))\n",
    "p19_mse\n",
    "* 自定义损失函数：$loss(y'y)=\\sum_{n}f(y'y)$  其中y'为标准答案数据集，y为预测答案计算出的。\n",
    "p20_custom.py\n",
    "* 交叉熵损失函数CE（Cross Entropy）:表征两个概率分布之间距离 $H(y',y)=-\\sum{y*lny}$  tf.losses.categorical_crossentropy(y',y)\n",
    "p22_ce.py\n",
    "* softmax与交叉熵结合，输出先过softmax函数，再计算y与y'的交叉熵损失函数\n",
    "    - tf.nn.softmax_cross_entropy_with_logist(y',y)\n",
    "\n",
    "#### 3.6 欠拟合与过拟合\n",
    "* 欠拟合的解决方法：\n",
    "    - 增加输入特征项\n",
    "    - 增加网络参数\n",
    "    - 减少正则化参数\n",
    "* 过拟合的解决方法：\n",
    "    - 数据清洗\n",
    "    - 增大训练集\n",
    "    - 采用正则化\n",
    "    - 增大正则化参数\n",
    "        \n",
    "#### 3.7 正则化减少过拟合\n",
    "* 正则化在损失函数中引入模型复杂度指标，利用给W加权值，弱化了训练数据的噪声（一般不正则化b）\n",
    "* loss = loss(y与y')+REGULARIZER*loss(w)\n",
    "    - 其中loss(y与y')是模型中所有参数的损失函数，如：交叉熵、均方误差\n",
    "    - 用超参数REGULARIZER给出参数w在总loss中的比例，即正则化的权重\n",
    "    - loss(w)需要正则化的参数\n",
    "* 正则化的选择 $loss_{L1}{(w)}=\\sum_{i}{|w_i|}$, $loss_{L2}{(w)}=\\sum_{i}{|w_i^2|}$\n",
    "    - L1正则化：大概率会使很多参数变为0,因此该方法可通过稀疏参数，即减少参数的数量，降低复杂度\n",
    "    - L2正则化：会使参数很接近于0但不为0，因此该方法可以通过减少参数值的大小降低复杂度。\n",
    "\n",
    "#### 3.8 优化器更新网络参数\n",
    "* 待优化参数w，损失函数loss, 学习率lr,每次迭代一个batch, 表示当前batch迭代的总次数\n",
    "    - 1.计算t时刻损失函数关于当前参数的梯度 $g_t=\\Delta{loss}=\\frac{\\partial{loss}}{\\partial{w_t}}$\n",
    "    - 2.计算t时刻一阶动量$m_t$和二阶动量$V_t$\n",
    "    - 3.计算t时刻下降梯度：$\\eta_t=\\frac{l_r*m_t}{\\sqrt{V_t}}$\n",
    "    - 4.计算t+1时刻参数 $w_{t+1}=w_t-\\eta_t=w_t-\\frac{l_r*m_t}{\\sqrt{V_t}}$\n",
    "        - 一阶动量：与梯度相关的函数\n",
    "        - 二阶动量：与梯度平方相关的函数\n",
    "* 常用网络优化器\n",
    "    - SGD(无momentum)，常用的梯度下降法\n",
    "       - $m_t=g_t$\n",
    "       - $V_t=1$\n",
    "       - $\\eta_t=\\frac{l_r*m_t}{\\sqrt{V_t}}=l_r*g_t$\n",
    "       - $w_{t+1}=w_t-l_r*\\frac{\\partial{loss}}{\\partial{w_t}}$\n",
    "           - w1.assign_sub(lr*grads[0])\n",
    "           - w2.assign_sub(lr*grads[1])\n",
    "           \n",
    "    - SGDM(含momentum的SGD),在SGD基础上增加一阶动量  p33_sgdm.py\n",
    "       - $m_t=\\beta*{m_{t-1}}+(1-\\beta)*g_t$\n",
    "       - $V_t$=1\n",
    "       - $\\eta_t=l_r*\\frac{m_t}{\\sqrt{V_t}}=l_r*m_t=l_r*(\\beta*m_{t-1}+(1-\\beta)*g_t)$\n",
    "       - $w_{t+1}=w_t-\\eta_t=w_t-l_r*(\\beta*m_{t-1}+(1-\\beta)*g_t)$\n",
    "           - m_w,m_b=0,0\n",
    "           - beta=0.9\n",
    "           - m_w=beta*m_t+(1-beta)*grad[0]\n",
    "           - m_b=beta*m_b+(1-beta)*grad[1]\n",
    "           - w1.assign_sub(lr*m_w)\n",
    "           - b1.assign_sub(lr*m_b)\n",
    "    - Adagrad,在SGD基础上增加二阶动量  p36.adagrad.py\n",
    "        - $m_t=g_t$\n",
    "        - $V_t=\\sum_{\\tau=1}^{t}{g_{\\tau}^2}$\n",
    "        - $\\eta_t=l_r*\\frac{m_t}{sqrt{V_t}}=l_r*\\frac{g_t}{\\sum_{\\tau=1}^{t}{g_{\\tau}^2}}$\n",
    "        - $w_{t+1}=w_t-\\eta_t=w_t-l_r*\\frac{g_t}{\\sum_{\\tau=1}^{t}{g_{\\tau}^2}}$\n",
    "           - v_w,v_b=0,0\n",
    "           - v_w+=tf.square(grads[0])\n",
    "           - v_b+=tf.square(grads[1])\n",
    "           - w1.assgin_sub(lr*grads[0]/tf.sqrt(v_w))\n",
    "           - b1.assgin_sub(lr*grads[1]/tf.sqrt(v_b))\n",
    "           \n",
    "     - RMSProp, SGD基础上增加二阶动量  p38_rmsprop.py\n",
    "        - $m_t=g_t$\n",
    "        - $V_t=\\beta*V_{t-1}+(1-\\beta)*g_t^2$    \n",
    "        - $\\eta_t=l_r*\\frac{m_t}{\\sqrt{V_t}}=l_r*\\frac{g_t}{\\sqrt{\\beta*V_{t-1}+(1+\\beta)*g_t^2}}$\n",
    "        - $w_{t+1}=w_t-\\eta_t=w_t-l_r*\\frac{g_t}{\\sqrt{\\beta*V_{t-1}+(1+\\beta)*g_t^2}}$\n",
    "           - m_w,m_b=0,0\n",
    "           - beta=0.9\n",
    "           - m_w=beta*m_t+(1-beta)*tf.square(grads[0])\n",
    "           - m_b=beta*m_b+(1-beta)*tf.square(grads[1])\n",
    "           - w1.assgin_sub(lr*grads[0]/tf.sqrt(v_w))\n",
    "           - b1.assgin_sub(lr*grads[1]/tf.sqrt(v_b))\n",
    "           \n",
    "     - Adam，同时结合SGDM一阶动量和RMSProp二阶动量   p40_adam.py\n",
    "         - $m_t=\\beta*m_{t-1}+(1-\\beta_1)*g_t$\n",
    "             - 修正一阶动量的偏差：$\\hat(m_t)=\\frac{m_t}{1-\\beta_1^t}$\n",
    "         - $V_t=\\beta_2*V_(step-1)+(1-\\beta_2)*g_t^2$\n",
    "             - 修正二阶动量的偏差：$\\hat(V_t)=\\frac{V_t}{1-\\beta_2^t}$\n",
    "         - $\\eta_t=l_r*\\frac{\\hat{m_t}}{\\sqrt{\\hat{V_t}}}=l_r*\\frac{\\frac{m_t}{1-\\beta_1^t}}{\\sqrt{\\frac{V_t}{1-\\beta_2^t}}}$\n",
    "         - $w_{t+1}=w_t-\\eta_t=w_t-l_r*\\frac{\\frac{m_t}{1-\\beta_1^t}}{\\sqrt{\\frac{V_t}{1-\\beta_2^t}}}$\n",
    "             - m_w,m_b=0,0\n",
    "             - v_w,v_b=0,0\n",
    "             - beta1,beta2=0.9,0.999\n",
    "             - delta_w,delta_b=0,0\n",
    "             - global_step=0\n",
    "             - m_w=beta1*m_w+(1-beta1)*grads[0]\n",
    "             - m_b=beta1*m_b+(1-beta1)*grads[1]\n",
    "             - v_w=beta2*v_w+(1-beta2)*tf.square(grads[0])\n",
    "             - v_b=beta2*v_b+(1-beta2)*tf.square(grads[1])\n",
    "             - m_w_correction=m_w/(1-tf.pow(beta1,int(global_step)))\n",
    "             - m_b_correction=m_b/(1-tf.pow(beta1,int(global_step)))\n",
    "             - v_w_correction=v_w/(1-tf.pow(beta1,int(global_step)))\n",
    "             - v_b_correction=v_b/(1-tf.pow(beta1,int(global_step)))\n",
    "             - w1.assgin_sub(lr*m_w_correction/tf.sqrt(v_w_correction))\n",
    "             - b1.assgin_sub(lr*m_b_correction/tf.sqrt(v_b_correction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 搭建网络八股总揽\n",
    "### 1. 自制数据集（解决本领域应用）\n",
    "\n",
    "### 2. 数据增强（扩充数据集）\n",
    "* image_gen_train = tf.keras.preprocessing.image.ImageDataGenerator(rescale=所有数据将乘以该数值，rotation_range=随机旋转角度数范围，width_shift_range=随机宽度偏移量，height_shift_range=随机高度偏移量，水平翻转：horizontal_flip=是否随机水平翻转，随机缩放：zoom_range=随机缩放的范围[1-n,1+n])\n",
    "* image_gen_train.fit(x_train)\n",
    "\n",
    "### 3. 断点续训（存取模型）\n",
    "* 读取模型: load_weights(路径文件名)\n",
    "* 保存模型: tf.keras.callbacks.ModelCheckpoint(filepath=路径文件名，save_weights_only=True/False,save_best_only=True/False)\n",
    "* history = model.fit(callbacks=[cp_callback])\n",
    "\n",
    "### 4. 参数提取（把参数存入文本）\n",
    "* 提取可训练参数： model.trainable_variables 返回模型中可训练的参数\n",
    "* 设置print输出格式： np.set_printoptions(threshold=超过多少省略显示)\n",
    "### 5. acc&loss可视化（查看训练效果）\n",
    "* history=model.fit(训练集数据，训练集标签，batch_size=,epochs=,validation_split=用作测试数据的比例，validation_data=测试集，validation_freq=测试频率)\n",
    "* history:\n",
    "    - 训练集loss: loss\n",
    "    - 测试集loss: val_loss\n",
    "    - 训练集准确率: sparse_categorical_accuracy\n",
    "    - 测试集准确率: val_sparse_categorical_accuracy\n",
    "\n",
    "### 6. 应用程序（给图识物）\n",
    "* predict(输入特征，batch_size=整数) 返回前向传播计算结果"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow] *",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
