{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 卷积网络 CNN\n",
    "* 全连接NN: 每个神经元与前后相邻层的每一个神经元都有连接关系，输入是特征，输出是与测结果。\n",
    "    - 参数个数：$\\sum_{各层}{(前层*后层(w)+后层(b))}$\n",
    "    - 如果输入特征过多，隐藏层过多，导致网络过度复杂，待优化的参数过多容易导致模型过拟合\n",
    "### 用CNN实现离散数据的分裂（以图像分类为例）\n",
    "* 卷积计算过程（Convolutional）\n",
    "    - 卷积计算可认为是一种有效提取图像特征的方法\n",
    "    - 一般会用一个正方形的卷积核，按指定步长，在输入特征图上华东，遍历输入特征图中的每个像素点。每一个步长，卷积核会与输入特征图出现重合区域，重合区域对应元素相乘、求和再加上偏置项得到输入特征的一个像素点\n",
    "    - 输入特征图的深度（channel数），决定了当前层卷积核的深度。\n",
    "    - 当前层卷积核的个数，决定了当前层输出特征图的深度。\n",
    "* 感受野（Receptive Field）：卷积神经网络各输出特征图中的每个像素点，在原始输入图片上映射区域的大小\n",
    "* 全零填充（Padding）：保证输入图与输出特征图像素点一致，TF描述全零填充用参数padding='SAME'或padding='VAILD '\n",
    "* TF描述卷积计算层\n",
    "    - tf.keras.layers.Conv2D(\n",
    "        - filters=卷积核个数，\n",
    "        - kernel_size=卷积核尺寸，#正方形写核长整数，或 用元组形式给出（核高h,核宽w），\n",
    "        - strides=滑动步长，#横纵向相同写步长整数，或（纵向步长h,横向步长w）,默认1)\n",
    "        - padding=\"same\"or\"valid\" #使用全零填充是same,不实用是valid(默认)\n",
    "        - activation=\"relu\"or\"sigmoid\"or\"tanh\"or\"softmax\"等，#如有BN此处不写\n",
    "        - input_sharp=(高，宽，通道数) #输入特征图维度，可省略\n",
    "    - )\n",
    "* 批标准化（Batch Normalization,BN）\n",
    "    - 标准化：使数据符合0均值，1为标准差的分布\n",
    "    - 批标准化：对一小批数据（batch）,做标准化处理\n",
    "        - 为每个卷积核引入可训练参数$\\gamma$和$\\beta$\n",
    "        - BN层位于卷积层之后，激活层之前\n",
    "        - TF描述批标准化：tf.keras.layers.BatchNormalization()\n",
    "            - model=tf.keras.models.Sequential([\n",
    "            - Conv2D(filter=6,kernel_size=(5,5),padding='same') #卷积层\n",
    "            - BatchNormalization(), #BN层\n",
    "            - Activation（‘relu’）,#激活层\n",
    "            - MaxPool2D(pool_size=(2,2),strides=2,padding='same'),#池优化\n",
    "            - Dropout(0.2), #dropout层\n",
    "            - ])\n",
    "        \n",
    "* 池化（Pooling）: 池化用于减少特征数据量。最大池化可以提取图片纹理，均值池化可以保留背景特征。\n",
    "    - TF描述池化\n",
    "        - tf.keras.layers.MaxPool2D(\n",
    "        - pool_size=池化核尺寸，#正方形写核长整数，或（高h,宽w）\n",
    "        - strides=池化步长， #步长整数，或（纵向步长h,横向步长w）\n",
    "        - padding='valid'或'same', #使用全零填充为'same',不使用为'valid'（默认）\n",
    "        - )\n",
    "        \n",
    "        - tf.keras.layers.AveragePooling2D(\n",
    "        - pool_size=池化核尺寸，#正方形写核长整数，或（高h,宽w）\n",
    "        - strides=池化步长， #步长整数，或（纵向步长h,横向步长w）\n",
    "        - padding='valid'或'same', #使用全零填充为'same',不使用为'valid'（默认）\n",
    "        - )\n",
    "* 舍弃（Dropout）:在神经网络训练时，将一部分神经元按照一定概率从神经网络中暂时舍弃。神经网络使用时，被舍弃的神经元回复链接。\n",
    "    - TF描述舍弃: tf.keras.layers.Dropout(舍弃的概率 )\n",
    "        \n",
    "* 卷积神经网络：借助卷积核提取特征后，送入券链接网络\n",
    "    - 卷积神经网络的主要模块（1，2，3，4步）CBAPD\n",
    "        - 1.卷积(Convolutional)---2.批标准化（BN）---3.激活（activation）----4.池化(Pooling)----5.全连接(fc)\n",
    "    \n",
    "* cifar10数据集\n",
    "    - 提供5万张32*32像素点的十分类彩色图片核标签，用于训练\n",
    "    - 提供1万张32*32像素点的十分类彩色图片核标签，用于测试\n",
    "    - cifar10=tf.keras.datasets.cifar10\n",
    "    - (xtrain,y_train),(x_test,y_test)=cifar10.load_data()\n",
    "* 卷积神经网络搭建示例 p27_cifar10.baseline.py\n",
    "* 实现LeNet(1998)、AlexNet(2012)、VGGNet(2014)、IneptionNet(2014)、ResNet(2015)五个经典卷积网络\n",
    "    - LeNet由Yann LeCun于1998年提出，卷积网络开篇之作。p31_cifar10_lenet5.py\n",
    "    - AlexNet网络诞生于2012年，当年ImageNet竞赛的冠军，Top5错误率为16.4%。p34_cifar10_alexnet8.py\n",
    "    - VGGNet诞生于2014年，当年ImageNet竞赛的亚军，Top5错误率减少到7.3%，网络非常适合硬件加速。p36_cifar10_vgg16.py\n",
    "    - InceptionNet诞生于2014年，当年ImageNet竞赛的冠军，Top5错误率减少到6.67%，在同一层网络内使用了不同尺寸的卷积核，提升了模型感知力，使用BN缓解了梯度消失。通过设定少于输入特征图深度的1*1卷积核个数减少了输出特征图深度，达到了降维的作用，减少了参数量核计算量。p40_cifar10_inception10.py\n",
    "    - ResNet诞生于2015年，当年ImageNet竞赛的冠军，Top5错误率减少到3.57%，提出了层间残差跳连，引入了前方信息，缓解梯度消失，使神经网络层数增加成为可能。p46_cifar10_resnet18.py\n",
    "\n",
    "|模型名称|网络层数|\n",
    "|-------|-------|\n",
    "|LeNet  |5      |\n",
    "|AlexNet|8\n",
    "|VGG|16/19|\n",
    "|InceptionNet v1|22|\n",
    "\n",
    "### 循环神经网络RNN实现连续数据的预测（以股票数据为例）\n",
    "\n",
    "#### 循环神经网络\n",
    "    * 循环核: 参数时间共享，循环层提取时间信息\n",
    "        - 前向传播时：记忆体内存储的状态信息ht,在每个时刻都被刷新，三个参数矩阵wxh whh why自始至终都是固定不变的。\n",
    "        - 反向传播时：三个参数矩阵wxh whh why被梯度下降法更新\n",
    "        \n",
    "$y_t = softmax(h_tw_{hy}+by)$\n",
    "\n",
    "$h_t = tanh(x_tw_{xh}+h_{t-1}w_{hh}+bh)$\n",
    "        \n",
    "    * 循环核时间步展开: 就是把循环核按照时间轴方向展开\n",
    "        - 循环神经网络：借助循环核提取时间特征后，送入全连接网络。\n",
    "    * 循环计算层： 向输出方向生长\n",
    "    * TF描述循环计算层\n",
    "        - tf.keras.layers.SimpleRNN(记忆体个数，activation='激活函数')\n",
    "        - return_sequences=是否每个时刻输出ht到下一层\n",
    "        - activation=\"激活函数\"，默认为tanh\n",
    "        - return_sequences= True 各时间步输出ht\n",
    "        - return_sequences= False 仅最后时间输出ht(默认)\n",
    "        \n",
    "        - 入RNN时，x_train维度：[送入样本数，循环核时间展开步数，每个时间步输入特征个数]\n",
    "        \n",
    "#### 实践：ABCDE字母预测\n",
    "    * One-hot：数据量大，过于稀疏，映射之间是独立的，没有表现出关联性\n",
    "    * Embedding：一种单词编码方法，用低维向量实现编码，这种编码通过神经网络训练优化，能表达出单词间的相关性\n",
    "        - tf.keras.layers.Embedding(词汇表大小，编码维度)\n",
    "        - 编码维度就是用几个数字表达一个单词\n",
    "        - 对1-100进行编码，【4】编码为[0.25,0.1,0.11]\n",
    "        - 例：tf.keras.layers.Embedding(100,3)\n",
    "        \n",
    "        - 入Embedding时，x_train维度：[送入样本数，循环核时间展开步数]\n",
    "    \n",
    "#### 实践：股票预测\n",
    "    * RNN 循环神经网络\n",
    "    * LSTM 长短记忆网络\n",
    "        - LSTM计算过程\n",
    "            - 输入门（门限）：\n",
    "$i_t = \\sigma{(w_i*[h_{t-1},x_t]+b_i)}$\n",
    "            - 遗忘门（门限）：\n",
    "$f_t = \\sigma{(w_f*[h_{t-1},x_t]+b_f)}$\n",
    "            - 输出门（门限）：\n",
    "$o_t = \\sigma{(w_o*[h_{t-1},x_t]+b_o)}$   \n",
    "            - 细胞态（长期记忆）：\n",
    "$C_t = f_t*C_{t-1}+i_t*\\widetilde{C_t}$    \n",
    "            - 记忆体（短期记忆）：\n",
    "$h_t = o_t*tanh(C_t)$\n",
    "            - 候选态（归纳出新的知识）：\n",
    "$\\widetilde{C_t}=tanh(W_c*[h_{t-1},x_t]+b_c)$\n",
    "\n",
    "        - tf.keras.layers.LSTM(记忆体个数，return_senquence=是否返回输出)\n",
    "            - retrun_sequences= True 各时间步输出ht\n",
    "            - return_sequences= False 仅最后时间步输出ht(默认)\n",
    "    * GRU\n",
    "        - GRU网络计算过程\n",
    "            - 更新门：\n",
    "$z_t=\\sigma(W_z*[h_{t-1},x_t])$\n",
    "            - 重置门：\n",
    "$r_t=\\sigma(W_r*[h_{t-1},x_t])$\n",
    "            - 记忆体：\n",
    "$h_t=(1-z_t)*h_{t-1}+z_t*\\widetilde{h_t}$\n",
    "            - 候选隐藏层：\n",
    "$\\widetilde{h_t}=tanh(W*[r_t*h_{t-1},x_t])$\n",
    "        \n",
    "        - tf.keras.layers.GRU(记忆体个数，return_sequences=是否返回输出)\n",
    "            - retrun_sequences= True 各时间步输出ht\n",
    "            - return_sequences= False 仅最后时间步输出ht(默认)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
